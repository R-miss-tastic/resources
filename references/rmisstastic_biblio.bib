% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{abayomi_etal_JRSSC2008,
  Title                    = {Diagnostics for multivariate imputations},
  Author                   = {Abayomi, K. and Gelman, A. and Levy, M.},
  Journal                  = {Journal of the Royal Statistical Society, Series C (Applied Statistics)},
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {273-291},
  Volume                   = {57},

  Abstract                 = {We consider three sorts of diagnostics for random imputations: displays of the completed data, which are intended to reveal unusual patterns that might suggest problems with the imputations, comparisons of the distributions of observed and imputed data values and checks of the fit of observed data to the model that is used to create the imputations. We formulate these methods in terms of sequential regression multivariate imputation, which is an iterative procedure in which the missing values of each variable are randomly imputed conditionally on all the other variables in the completed data matrix. We also consider a recalibration procedure for sequential regression imputations. We apply these methods to the 2002 environmental sustainability index, which is a linear aggregation of 64 environmental variables on 142 countries.},
  Doi                      = {10.1111/j.1467-9876.2007.00613.x},
  ISSN                     = {1467-9876},
  Keywords                 = {missing values; multiple imputation: multivariate statistics; sustainability; environmental statistics},
  Owner                    = {alyssa},
  Publisher                = {Blackwell Publishing Ltd},
  Timestamp                = {2017.11.08},
  Topics                   = {diagnosis}
}

@Article{albert_follmann_B2000,
  Title                    = {Modeling repeated count data subject to informative dropout},
  Author                   = {Albert, P.S. and Follmann, D.A.},
  Journal                  = {Biometrics},
  Year                     = {2000},
  Number                   = {3},
  Pages                    = {667-677},
  Volume                   = {56},

  Abstract                 = {In certain diseases, outcome is the number of morbid events over the course of follow-up. In epilepsy, e.g., daily seizure counts are often used to reflect disease severity. Follow-up of patients in clinical trials of such diseases is often subject to censoring due to patients dying or dropping out. If the sicker patients tend to be censored in such trials, estimates of the treatment effect that do not incorporate the censoring process may be misleading. We extend the shared random effects approach of Wu and Carroll (1988, Biometrics 44, 175-188) to the setting of repeated counts of events. Three strategies are developed. The first is a likelihood-based approach for jointly modeling the count and censoring processes. A shared random effect is incorporated to introduce dependence between the two processes. The second is a likelihood-based approach that conditions on the dropout times in adjusting for informative dropout. The third is a generalized estimating equations (GEE) approach, which also conditions on the dropout times but makes fewer assumptions about the distribution of the count process. Estimation procedures for each of the approaches are discussed, and the approaches are applied to data from an epilepsy clinical trial. A simulation study is also conducted to compare the various approaches. Through analyses and simulations, we demonstrate the flexibility of the likelihood-based conditional model for analyzing data from the epilepsy trial.},
  Doi                      = {10.2307/2676907},
  ISSN                     = {0006341X, 15410420},
  Owner                    = {alyssa},
  Publisher                = {[Wiley, International Biometric Society]},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar}
}

@Book{allison_MD2001,
  Title                    = {Missing Data},
  Author                   = {Allison, P.D.},
  Publisher                = {Sage Publications},
  Year                     = {2001},

  Address                  = {Thousand Oaks, CA, USA},
  Series                   = {Quantitative Applications in the Social Sciences},

  Doi                      = {10.1136/bmj.38977.682025.2C},
  ISBN                     = {9780761916727},
  ISSN                     = {0959-8138},
  Mendeley-groups          = {missing data},
  Owner                    = {nathalie},
  Timestamp                = {2017.03.06},
  Topics                   = {general}
}

@Article{andridge_little_ISR2010,
  Title                    = {A review of hot deck imputation for survey non-response},
  Author                   = {Andridge, R. and Little, R.J.A.},
  Journal                  = {International Statistical Review},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {40-64},
  Volume                   = {78},

  Abstract                 = {Hot deck imputation is a method for handling missing data in which each missing value is replaced with an observed response from a ``similar'' unit. Despite being used extensively in practice, the theory is not as well developed as that of other imputation methods. We have found that no consensus exists as to the best way to apply the hot deck and obtain inferences from the completed data set. Here we review different forms of the hot deck and existing research on its statistical properties. We describe applications of the hot deck currently in use, including the U.S. Census Bureau's hot deck for the Current Population Survey (CPS). We also provide an extended example of variations of the hot deck applied to the third National Health and Nutrition Examination Survey (NHANES III). Some potential areas for future research are highlighted.},
  Annote                   = {A review of Hot deck imputation for survey Non-response},
  Doi                      = {10.1111/j.1751-5823.2010.00103.x.A},
  Keywords                 = {item non-response,missing data,multiple imputation,variance estimation},
  Mendeley-groups          = {missing data},
  Owner                    = {alyssa},
  Timestamp                = {2016.09.27},
  Topics                   = {hot-deck; imputation}
}

@Article{audigier_etal_ADAC2016,
  Title                    = {A principal component method to impute missing values for mixed data},
  Author                   = {Audigier, V. and Husson, F. and Josse, J.},
  Journal                  = {Advances in Data Analysis and Classification},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {5-26},
  Volume                   = {10},

  Abstract                 = {We propose a new method to impute missing values in mixed data sets. It is based on a principal component method, the factorial analysis for mixed data, which balances the influence of all the variables that are continuous and categorical in the construction of the principal components. Because the imputation uses the principal axes and components, the prediction of the missing values is based on the similarity between individuals and on the relationships between variables. The properties of the method are illustrated via simulations and the quality of the imputation is assessed using real data sets. The method is compared to a recent method (Stekhoven and Buhlmann Bioinformatics 28:113_118, 2011) based on random forest and shows better performance especially for the imputation of categorical variables and situations with highly linear relationships between continuous variables.},
  Doi                      = {10.1007/s11634-014-0195-1},
  Keywords                 = {missing values, mixed data, imputation, principal component method, factorial analysis of mixed data},
  Owner                    = {alyssa},
  Timestamp                = {2017.02.22},
  Topics                   = {factorial data analysis; imputation}
}

@Article{audigier_etal_SC2016,
  Title                    = {{MIMCA}: multiple imputation for categorical variables with multiple correspondence analysis},
  Author                   = {Audigier, V. and Husson, F. and Josse, J.},
  Journal                  = {Statistics and Computing},
  Year                     = {2016},
  Number                   = {2},
  Pages                    = {1-18},
  Volume                   = {27},

  Abstract                 = {We propose a multiple imputation method to deal with incomplete categorical data. This method imputes the missing entries using the principal components method dedicated to categorical data: multiple correspondence analysis {\{}(MCA).{\}} The uncertainty concerning the parameters of the imputation model is reflected using a non-parametric bootstrap. Multiple imputation using {\{}MCA{\}} {\{}(MIMCA){\}} requires estimating a small number of parameters due to the dimensionality reduction property of {\{}MCA.{\}} It allows the user to impute a large range of data sets. In particular, a high number of categories per variable, a high number of variables or a small the number of individuals are not an issue for {\{}MIMCA.{\}} Through a simulation study based on real data sets, the method is assessed and compared to the reference methods (multiple imputation using the loglinear model, multiple imputation by logistic regressions) as well to the latest works on the topic (multiple imputation by random forests or by the Dirichlet process mixture of products of multinomial distributions model). The proposed method shows good performances in terms of bias and coverage for an analysis model such as a main effects logistic regression model. In addition, {\{}MIMCA{\}} has the great advantage that it is substantially less time consuming on data sets of high dimensions than the other multiple imputation methods.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1505.08116},
  Doi                      = {10.1007/s11222-016-9635-4},
  Eprint                   = {1505.08116},
  ISSN                     = {15731375},
  Keywords                 = {Bootstrap,Categorical data,Missing values,Multiple correspondence analysis,Multiple imputation},
  Owner                    = {alyssa},
  Publisher                = {Springer US},
  Timestamp                = {2017.07.06},
  Topics                   = {factorial data analysis; multiple imputation}
}

@Article{audigier_etal_JSCS2015,
  Title                    = {Multiple imputation for continuous variables using a {B}ayesian principal component analysis},
  Author                   = {Audigier, V. and Husson, F. and Josse,J.},
  Journal                  = {Journal of Statistical Computation and Simulation},
  Year                     = {2015},
  Number                   = {11},
  Pages                    = {2140-2156},
  Volume                   = {86},

  Abstract                 = {We propose a multiple imputation method based on principal component analysis (PCA) to deal with incomplete continuous data. To reflect the uncertainty of the parameters from one imputation to the next, we use a Bayesian treatment of the PCA model. Using a simulation study and real data sets, the method is compared to two classical approaches: multiple imputation based on joint modelling and on fully conditional modelling. Contrary to the others, the proposed method can be easily used on data sets where the number of individuals is less than the number of variables and when the variables are highly correlated. In addition, it provides unbiased point estimates of quantities of interest, such as an expectation, a regression coefficient or a correlation coefficient, with a smaller mean squared error. Furthermore, the widths of the confidence intervals built for the quantities of interest are often smaller whilst ensuring a valid coverage.},
  Doi                      = {10.1080/00949655.2015.1104683},
  Keywords                 = {missing values, continuous data, multiple imputaiton, Bayesian principal component analysis, data augmentation},
  Owner                    = {alyssa},
  Timestamp                = {2017.02.23},
  Topics                   = {factorial data analysis; multiple imputation}
}

@Article{bang_robins_B2005,
  Title                    = {Doubly robust estimation in missing data and causal inference models},
  Author                   = {Bang, H. and Robins, J.M.},
  Journal                  = {Biometrics},
  Year                     = {2005},
  Number                   = {4},
  Pages                    = {962-973},
  Volume                   = {61},

  Abstract                 = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
  Doi                      = {10.1111/j.1541-0420.2005.00377.x},
  ISBN                     = {0006-341X},
  ISSN                     = {0006341X},
  Keywords                 = {causal inference; doubly robust estimation; longitudinal data; marginal structural model; missing data; semiparametrics},
  Owner                    = {alyssa},
  Pmid                     = {16401269},
  Timestamp                = {2017.05.29}
}

@Article{baraldi_enders_JSP2010,
  Title                    = {An introduction to modern missing data analysis},
  Author                   = {Baraldi, A.N. and Enders, C.K.},
  Journal                  = {Journal of School Psychology},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {5-37},
  Volume                   = {48},

  Abstract                 = {A great deal of recent methodological research has focused on two modern missing data analysis methods: maximum likelihood and multiple imputation. These approaches are advantageous to traditional techniques (e.g. deletion and mean imputation techniques) because they require less stringent assumptions and mitigate the pitfalls of traditional techniques. This article explains the theoretical underpinnings of missing data analyses, gives an overview of traditional missing data techniques, and provides accessible descriptions of maximum likelihood and multiple imputation. In particular, this article focuses on maximum likelihood estimation and presents two analysis examples from the Longitudinal Study of American Youth data. One of these examples includes a description of the use of auxiliary variables. Finally, the paper illustrates ways that researchers can use intentional, or planned, missing data to enhance their research designs.},
  Doi                      = {10.1016/j.jsp.2009.10.001},
  Keywords                 = {missing data; multiple imputation; maximum likelihood; planned missingness},
  Owner                    = {alyssa},
  Timestamp                = {2017.02.21},
  Topics                   = {general}
}

@Article{baretta_santaniello_BMCMIDM2016,
  Title                    = {Nearest neighbor imputation algorithms: a critical evaluation},
  Author                   = {Baretta, L. and Santaniello, A.},
  Journal                  = {BMC Medical Informatics and Decision Making},
  Year                     = {2016},
  Number                   = {Supp. 3},
  Pages                    = {74},
  Volume                   = {16},

  Abstract                 = {Background Nearest neighbor (NN) imputation algorithms are efficient methods to fill in missing data where each missing value on some records is replaced by a value obtained from related cases in the whole set of records. Besides the capability to substitute the missing data with plausible values that are as close as possible to the true value, imputation algorithms should preserve the original data structure and avoid to distort the distribution of the imputed variable. Despite the efficiency of NN algorithms little is known about the effect of these methods on data structure. Methods Simulation on synthetic datasets with different patterns and degrees of missingness were conducted to evaluate the performance of NN with one single neighbor (1NN) and with k neighbors without (kNN) or with weighting (wkNN) in the context of different learning frameworks: plain set, reduced set after ReliefF filtering, bagging, random choice of attributes, bagging combined with random choice of attributes (Random-Forest-like method). Results Whatever the framework, kNN usually outperformed 1NN in terms of precision of imputation and reduced errors in inferential statistics, 1NN was however the only method capable of preserving the data structure and data were distorted even when small values of k neighbors were considered; distortion was more severe for resampling schemas. Conclusions The use of three neighbors in conjunction with ReliefF seems to provide the best trade-off between imputation error and preservation of the data structure. The very same conclusions can be drawn when imputation experiments were conducted on the single proton emission computed tomography (SPECTF) heart dataset after introduction of missing data completely at random.},
  Doi                      = {10.1186/s12911-016-0318-z},
  Keywords                 = {near neighbour; imputation method; imputation algorithm; near neighbour algorithm; Minkowski norm},
  Owner                    = {nathalie},
  Series                   = {Proceedings of the 5th Translational Bioinformatics Conference (TBC 2015): medical informatics and decision making},
  Timestamp                = {2018.05.17},
  Topics                   = {knn; imputation}
}

@Article{buck_JRSSB1960,
  Title                    = {A method of estimation of missing values in multivariate data suitable for use with an electronic computer},
  Author                   = {Buck, S.F.},
  Journal                  = {Journal of the Royal Statistical Society, Series B},
  Year                     = {1960},
  Pages                    = {302-306},
  Volume                   = {22},

  Abstract                 = {Procedures for treating missing data in the statistical analysis of survey data are reviewed. The main topics covered are: (1) how to assess the nature of missing data especially with regard to randomness, (2) a comparison of listwise and pairwise deletion, and (3) methods for using maximum information to estimate (a) parameters or (b) missing values.},
  Doi                      = {10.1177/004912417700600206},
  Owner                    = {nathalie},
  Timestamp                = {2016.09.28},
  Topics                   = {general}
}

@InProceedings{burns_ARC1990,
  Title                    = {Multiple and replicate item imputation in a complex sample survey},
  Author                   = {Burns, R.M.},
  Booktitle                = {Proceedings of the 6th Annual Research Conference},
  Year                     = {1990},

  Address                  = {Washington DC, USA},
  Editor                   = {Bureau of the Census},
  Pages                    = {655-665},

  Owner                    = {nathalie},
  Timestamp                = {2018.06.06}
}

@Book{vanbuuren_FIMD2012,
  Title                    = {Flexible Imputation of Missing Data},
  Author                   = {van Buuren, S.},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {2012},

  Address                  = {Leiden, The Netherlands},

  Abstract                 = {Missing data form a problem in every scientific discipline, yet the techniques required to handle them are complicated and often lacking. One of the great ideas in statistical science-multiple imputation-fills gaps in the data with plausible values, the uncertainty of which is coded in the data itself. It also solves other problems, many of which are missing data problems in disguise. Flexible Imputation of Missing Data is supported by many examples using real data taken from the author's vast experience of collaborative research, and presents a practical guide for handling missing data under the framework of multiple imputation. Furthermore, detailed guidance of implementation in R using the author's package MICE is included throughout the book. Assuming familiarity with basic statistical concepts and multivariate methods, Flexible Imputation of Missing Data is intended for two audiences: (Bio)statisticians, epidemiologists, and methodologists in the social and health sciences; Substantive researchers who do not call themselves statisticians, but who possess the necessary skills to understand the principles and to follow the recipes. This graduate-tested book avoids mathematical and technical details as much as possible: formulas are accompanied by a verbal statement that explains the formula in layperson terms. Readers less concerned with the theoretical underpinnings will be able to pick up the general idea, and technical material is available for those who desire deeper understanding. The analyses can be replicated in R using a dedicated package developed by the author.},
  Owner                    = {alyssa},
  Timestamp                = {2017.04.11},
  Topics                   = {general}
}

@Article{vanbuuren_SMMR2007,
  Title                    = {Multiple imputation of discrete and continuous data by fully conditional specification},
  Author                   = {van Buuren, Stef},
  Journal                  = {Statistical Methods in Medical Research},
  Year                     = {2007},
  Pages                    = {219-242},
  Volume                   = {16},

  Abstract                 = {The goal of multiple imputation is to provide valid inferences for statistical estimates from incomplete data. To achieve that goal, imputed values should preserve the structure in the data, as well as the uncertainty about this structure, and include any knowledge about the process that generated the missing data. Two approaches for imputing multivariate data exist: joint modeling (JM) and fully conditional specification (FCS). JM is based on parametric statistical theory, and leads to imputation procedures whose statistical properties are known. JM is theoretically sound, but the joint model may lack flexibility needed to represent typical data features, potentially leading to bias. FCS is a semi-parametric and flexible alternative that specifies the multivariate model by a series of conditional models, one for each incomplete variable. FCS provides tremendous flexibility and is easy to apply, but its statistical properties are difficult to establish. Simulation work shows that FCS behaves very well in the cases studied. The present paper reviews and compares the approaches. JM and FCS were applied to pubertal development data of 3801 Dutch girls that had missing data on menarche (two categories), breast development (five categories) and pubic hair development (six stages). Imputations for these data were created under two models: a multivariate normal model with rounding and a conditionally specified discrete model. The JM approach introduced biases in the reference curves, whereas FCS did not. The paper concludes that FCS is a useful and easily applied flexible alternative to JM when no convenient and realistic joint distribution can be specified.},
  Doi                      = {10.1177/0962280206074463},
  Owner                    = {alyssa},
  Timestamp                = {2016.09.27},
  Topics                   = {multiple imputation; FCS}
}

@Article{vanbuuren_etal_JSCS2006,
  Title                    = {Fully conditional specification in multivariate imputation},
  Author                   = {van Buuren, S. and Brand, J.P.L. and Groothuis-Oudshoorn, C.G.M. and Rubin, D.B.},
  Journal                  = {Journal of Statistical Computation and Simulation},
  Year                     = {2006},
  Number                   = {12},
  Pages                    = {1049-1064},
  Volume                   = {76},

  Doi                      = {10.1080/10629360600810434},
  Owner                    = {nathalie},
  Timestamp                = {2016.09.28},
  Topics                   = {FCS; multiple imputation}
}

@Article{vanbuuren_groothuisoudshoorn_JSS2011,
  Title                    = {{MICE}: multivariate imputation by chained equations in {R}},
  Author                   = {van Buuren, S. and Groothuis-Oudshoorn, K.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2011},
  Pages                    = {3},
  Volume                   = {45},

  Abstract                 = {The R package mice imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice 2.9, which extends the functionality of mice 1.0 in several ways. In mice 2.9, the analysis of imputed data is made completely general, whereas the range ofmodels under which pooling works is substantially extended. mice 2.9 adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. mice 2.9 can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {NIHMS150003},
  Doi                      = {10.18637/jss.v045.i03},
  Eprint                   = {NIHMS150003},
  ISBN                     = {9067436771},
  ISSN                     = {1548-7660},
  Owner                    = {alyssa},
  Pmid                     = {22289957},
  Timestamp                = {2017.10.16},
  Topics                   = {multiple imputation; chained equations}
}

@Article{candes_etal_IEEETSP2013,
  Title                    = {Unbiased risk estimates for singular value thresholding and spectral estimators},
  Author                   = {Cand\`es, E.J. and Sing-Long, C.A. and Trzasko, J.D.},
  Journal                  = {IEEE Transactions on Signal Processing},
  Year                     = {2013},
  Number                   = {19},
  Pages                    = {4643-4657},
  Volume                   = {61},

  Abstract                 = {In an increasing number of applications, it is of interest to recover an approximately low-rank data matrix from noisy observations. This paper develops an unbiased risk estimate -- holding in a Gaussian model -- for any spectral estimator obeying some mild regularity assumptions. In particular, we give an unbiased risk estimate formula for singular value thresholding (SVT), a popular estimation strategy that applies a soft-thresholding rule to the singular values of the noisy observations. Among other things, our formulas offer a principled and automated way of selecting regularization parameters in a variety of problems. In particular, we demonstrate the utility of the unbiased risk estimation for SVT-based denoising of real clinical cardiac MRI series data. We also give new results concerning the differentiability of certain matrix-valued functions.},
  Doi                      = {10.1109/TSP.2013.2270464},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.09}
}

@Book{carpenter_kenward_MIA2013,
  Title                    = {Multiple Imputation and its Application},
  Author                   = {Carpenter, J. and Kenward, M.},
  Publisher                = {Wiley},
  Year                     = {2013},

  Abstract                 = {A practical guide to analysing partially observed data. Collecting, analysing and drawing inferences from data is central to research in the medical and social sciences. Unfortunately, it is rarely possible to collect all the intended data. The literature on inference from the resulting incomplete data is now huge, and continues to grow both as methods are developed for large and complex data structures, and as increasing computer power and suitable software enable researchers to apply these methods. This book focuses on a particular statistical method for analysing and drawing inferences from incomplete data, called Multiple Imputation (MI). MI is attractive because it is both practical and widely applicable. The authors aim is to clarify the issues raised by missing data, describing the rationale for MI, the relationship between the various imputation models and associated algorithms and its application to increasingly complex data structures.},
  Doi                      = {10.1002/9781119942283},
  Owner                    = {alyssa},
  Timestamp                = {2017.04.11},
  Topics                   = {multiple imputation; general}
}

@Article{chen_shao_JOS2000,
  Title                    = {Nearest neighbor imputation for survey data},
  Author                   = {Chen, J. and Shao, J.},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2000},
  Number                   = {2},
  Pages                    = {113-131},
  Volume                   = {16},

  Abstract                 = {Nearest neighbor imputation is one of the hot deck methods used to compensate for nonresponse in sample surveys. Although it has a long history of application, few theoretical properties of the nearest neighbor imputation method are known prior to the current article. We show that under some conditions, the nearest neighbor imputation method provides asymptotically unbiased and consistent estimators of functions of population means (or totals), population distributions, and population quantiles. We also derive the asymptotic variances for estimators based on nearest neighbor imputation and consistent estimators of these asymptotic variances. Some simulation results show that the estimators based on nearest neighbor imputation and the proposed variance estimators have good performances.},
  ISSN                     = {0282-423X},
  Keywords                 = {biases,hot deck,quantiles,sample means,variance estimation},
  Mendeley-groups          = {missing data},
  Owner                    = {alyssa},
  Timestamp                = {2016.09.27},
  Topics                   = {knn; imputation},
  Url                      = {http://www.jos.nu/Articles/abstract.asp?article=162113}
}

@Article{collins_etal_PM2007,
  Title                    = {A comparison of inclusive and restrictive strategies in modern missing data procedures},
  Author                   = {Collins, Linda M. and Schafer, Joseph L. and Chi-Ming, Karn},
  Journal                  = {Psychological Methods},
  Year                     = {2007},
  Number                   = {4},
  Pages                    = {330-351},
  Volume                   = {6},

  Abstract                 = {Two classes of modem missing data procedures, maximum likelihood (ML) and multiple imputation (MI), tend to yield similar results when implemented in comparable ways. In either approach, it is possible to include auxiliary variables solely for the purpose of improving the missing data procedure. A simulation was presented to assess the potential costs and benefits of a restrictive strategy, which makes minimal use of auxiliary variables, versus an inclusive strategy, which makes liberal use of such variables. The simulation showed that the inclusive strategy is to be greatly preferred. With an inclusive strategy not only is there a reduced chance of inadvertently omitting an important cause of missingness, there is also the possibility of noticeable gains in terms of increased efficiency and reduced bias, with only minor costs. As implemented in currently available software, the ML approach tends to encourage the use of a restrictive strategy, whereas the MI approach makes it relatively simple to use an inclusive strategy.},
  Doi                      = {10.1037/1082-989X.6.4.330},
  Owner                    = {nathalie},
  Timestamp                = {2018.06.06},
  Topics                   = {multiple imputation; ML}
}

@Article{cranmer_gill_BJPS2012,
  Title                    = {We have to be discrete about this: a non-parametric imputation technique for missing categorical data},
  Author                   = {Cranmer, S.J. and Gill, J.},
  Journal                  = {British Journal of Political Science},
  Year                     = {2012},
  Pages                    = {425-449},
  Volume                   = {43},

  Abstract                 = {Missing values are a frequent problem in empirical political science research. Surprisingly, the match between the measurement of the missing values and the correcting algorithms applied is seldom studied. While multiple imputation is a vast improvement over the deletion of cases with missing values, it is often unsuitable for imputing highly non-granular discrete data. We develop a simple technique for imputing missing values in such situations, which is a variant of hot deck imputation, drawing from the conditional distribution of the variable with missing values to preserve the discrete measure of the variable. This method is tested against existing techniques using Monte Carlo analysis and then applied to real data on democratization and modernization theory. Software for our imputation technique is provided in a free, easy-to-use package for the R statistical environment.},
  Doi                      = {10.1017/S0007123412000312},
  Owner                    = {nathalie},
  Timestamp                = {2016.02.15},
  Topics                   = {knn; imputation}
}

@Article{crookston_finley_JSS2008,
  Title                    = {{yaImpute}: an {R} package for {kNN} imputation},
  Author                   = {Crookston, N.L. and Finley, A.O.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2008},
  Pages                    = {10},
  Volume                   = {23},

  Abstract                 = {This article introduces yaImpute, an R package for nearest neighbor search and imputation. Although nearest neighbor imputation is used in a host of disciplines, the methods implemented in the yaImpute package are tailored to imputation-based forest attribute estimation and mapping. The impetus to writing the yaImpute is a growing interest in nearest neighbor imputation methods for spatially explicit forest inventory, and a need within this research community for software that facilitates comparison among different nearest neighbor search algorithms and subsequent imputation techniques. yaImpute provides directives for defining the search space, subsequent distance calculation, and imputation rules for a given number of nearest neighbors. Further, the package offers a suite of diagnostics for comparison among results generated from different imputation analyses and a set of functions for mapping imputation results.},
  Doi                      = {10.18637/jss.v023.i10},
  Owner                    = {nathalie},
  Timestamp                = {2017.10.09},
  Topics                   = {knn; imputation}
}

@Article{dempster_etal_JRSSB1977,
  Title                    = {Maximum likelihood from incomplete data via the {EM} algorithm},
  Author                   = {Dempster, A.P. and Laird, N.M. and Rubin, D.B.},
  Journal                  = {Journal of the Royal Statistical Society, Series B (Methodological)},
  Year                     = {1977},
  Number                   = {1},
  Pages                    = {1-38},
  Volume                   = {39},

  Keywords                 = {maximum likelihood estimation; statistical variance; statism; factor analysis; algorithms; estimation methods; missing data; censored data; perceptron convergence procedure},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.11},
  Topics                   = {ML},
  Url                      = {http://www.jstor.org/stable/2984875}
}

@Article{diggle_kenward_AP1994,
  Title                    = {Informative drop-out in longitudinal data analysis},
  Author                   = {Diggle, P. and Kenward, M.G.},
  Journal                  = {Journal of the Royal Statistical Society, Series C (Applied Statistics)},
  Year                     = {1994},
  Number                   = {1},
  Pages                    = {49-93},
  Volume                   = {43},

  Abstract                 = {A model is proposed for continuous longitudinal data with non-ignorable or informative drop-out (ID). The model combines a multivariate linear model for the underlying response with a logistic regression model for the drop-out process. The latter incorporates dependence of the probability of drop-out on unobserved, or missing, observations. Parameters in the model are estimated by using maximum likelihood (ML) and inferences drawn through conventional likelihood procedures. In particular, likelihood ratio tests can be used to assess the informativeness of the drop-out process through comparison of the full model with reduced models corresponding to random drop-out (RD) and completely random processes. A simulation study is used to assess the procedure in two settings: the comparison of time trends under a linear regression model with autocorrelated errors and the estimation of period means and treatment differences from a four-period four-treatment crossover trial. It is seen in both settings that, when data are generated under an ID process, the ML estimators from the ID model do not suffer from the bias that is present in the ordinary least squares and RD ML estimators. The approach is then applied to three examples. These derive from a milk protein trial involving three groups of cows, milk yield data from a study of mastitis in dairy cattle and data from a multicentre clinical trial on the study of depression. All three examples provide evidence of an underlying ID process, two with some strength. It is seen that the assumption of an ID rather than an RD process has practical implications for the interpretation of the data.},
  Doi                      = {10.2307/2986113},
  ISBN                     = {00359254},
  ISSN                     = {00359254, 14679876},
  Keywords                 = {longitudinal methods; missing data},
  Mendeley-groups          = {missing data},
  Owner                    = {alyssa},
  Pmid                     = {6121453},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar}
}

@Article{ding_simonoff_JMLR2010,
  Title                    = {An investigation of missing data methods for classification trees applied to binary response data},
  Author                   = {Ding, Y. and Simonoff, J.S.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2010},
  Pages                    = {131-170},
  Volume                   = {11},

  Abstract                 = {There are many different methods used by classification tree algorithms when missing data occur in the predictors, but few studies have been done comparing their appropriateness and performance. This paper provides both analytic and Monte Carlo evidence regarding the effectiveness of six popular missing data methods for classification trees applied to binary response data. We show that in the context of classification trees, the relationship between the missingness and the dependent variable, as well as the existence or non-existence of missing values in the testing data, are the most helpful criteria to distinguish different missing data methods. In particular, separate class is clearly the best method to use when the testing set has missing values and the missingness is related to the response variable. A real data set related to modeling bankruptcy of a firm is then analyzed. The paper concludes with discussion of adaptation of these results to logistic regression, and other potential generalizations.},
  Keywords                 = {classification tree; missing data; separate class; RPART; C4.5; CART},
  Owner                    = {nathalie},
  Timestamp                = {2016.11.30},
  Topics                   = {imputation; surrogate variables},
  Url                      = {http://www.jmlr.org/papers/v11/ding10a.html}
}

@Article{dong_peng_SP2013,
  Title                    = {Principled missing data methods for researchers},
  Author                   = {Dong, Yiran and Peng, Chao-Ying Joanne},
  Journal                  = {SpringerPlus},
  Year                     = {2013},
  Pages                    = {222},
  Volume                   = {2},

  Abstract                 = {The impact of missing data on quantitative research can be serious, leading to biased estimates of parameters, loss of information, decreased statistical power, increased standard errors, and weakened generalizability of findings. In this paper, we discussed and demonstrated three principled missing data methods: multiple imputation, full information maximum likelihood, and expectation-maximization algorithm, applied to a real-world data set. Results were contrasted with those obtained from the complete data set and from the listwise deletion method. The relative merits of each method are noted, along with common features they share. The paper concludes with an emphasis on the importance of statistical assumptions, and recommendations for researchers. Quality of research will be enhanced if (a) researchers explicitly acknowledge missing data problems and the conditions under which they occurred, (b) principled methods are employed to handle missing data, and (c) the appropriate treatment of missing data is incorporated into review standards of manuscripts submitted for publication.},
  Doi                      = {10.1186/2193-1801-2-222},
  Keywords                 = {missing data; listwise deletion; MI; FIML; EM; MAR; MCAR; MNAR},
  Owner                    = {nathalie},
  Timestamp                = {2018.06.06},
  Topics                   = {general}
}

@Book{enders_AMDA2010,
  Title                    = {Applied Missing Data Analysis},
  Author                   = {Enders, C.K.},
  Publisher                = {Guilford Press},
  Year                     = {2010},

  Abstract                 = {Walking readers step by step through complex concepts, this book translates missing data techniques into something that applied researchers and graduate students can understand and utilize in their own research. Enders explains the rationale and procedural details for maximum likelihood estimation, Bayesian estimation, multiple imputation, and models for handling missing not at random (MNAR) data. Easy-to-follow examples and small simulated data sets illustrate the techniques and clarify the underlying principles. The companion website includes data files and syntax for the examples in the book as well as up-to-date information on software. The book is accessible to substantive researchers while providing a level of detail that will satisfy quantitative specialists.},
  ISBN                     = {9781606236390},
  Owner                    = {alyssa},
  Pages                    = {401},
  Timestamp                = {2016.09.27},
  Topics                   = {general}
}

@Article{enders_SEM2001,
  Title                    = {A primer on maximum likelihood algorithms available for use with missing data},
  Author                   = {Enders, C.K.},
  Journal                  = {Structural Equation Modeling},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {128-141},
  Volume                   = {8},

  Abstract                 = {Maximum likelihood algorithms for use with missing data are becoming commonplace in microcomputer packages. Specifically, 3 maximum likelihood algorithms are currently available in existing software packages: the multiple-group approach, full information maximum likelihood estimation, and the EM algorithm. Although they belong to the same family of estimator, confusion appears to exist over the differences among the 3 algorithms. This article provides a comprehensive, nontechnical overview of the 3 maximum likelihood algorithms. Multiple imputation, which is frequently used in conjunction with the EM algorithm, is also discussed.},
  Doi                      = {10.1207/S15328007SEM0801_7},
  Owner                    = {alyssa},
  Timestamp                = {2017.07.07},
  Topics                   = {ML}
}

@Article{fay_JASA1996,
  Title                    = {Alternative paradigms for the analysis of imputed survey data},
  Author                   = {Fay, R.E.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1996},
  Number                   = {434},
  Pages                    = {490-498},
  Volume                   = {91},

  Abstract                 = {Rubin has offered multiple imputation as a general approach to inference from survey data sets with missing values filled in through imputation. In many situations the multiple imputation variance estimator is consistent. In tum, this observation has lent support to a number of complex applications. In fact, however, the multiple imputation variance estimator is inconsistent under some simple conditions. This article extends previous work of Rao and Shao and of Fay directed toward consistent variance estimation under wider conditions. Extensions of Rao and Shao's results to fractionally weighted imputation combines the estimation efficiency of multiple imputation and the consistency of the Rao-Shao variance estimator.},
  Doi                      = {10.1080/01621459.1996.10476909},
  Keywords                 = {fractionally weighted imputation; missing data; multiple imputation; Rao-Shao variance estimator},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.16},
  Topics                   = {multiple imputation}
}

@Article{fellegi_holt_JASA1976,
  Title                    = {A systematic approach to automatic edit and imputation},
  Author                   = {Fellegi, I.P. and Holt, D.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1976},
  Number                   = {353},
  Pages                    = {17-35},
  Volume                   = {71},

  Doi                      = {10.2307/2285726},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.23},
  Topics                   = {imputation}
}

@Article{ferrari_etal_CSDA2011,
  Title                    = {An imputation method for categorical variables with application to nonlinear principal component analysis},
  Author                   = {Ferrari, Pier Alda and Annoni, Paola and Barbiero, Alessandro and Manzi, Giancario},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2011},
  Number                   = {7},
  Pages                    = {2410-2420},
  Volume                   = {55},

  Abstract                 = {The problem of missing data in building multidimensional composite indicators is a delicate problem which is often underrated. An imputation method particularly suitable for categorical data is proposed. This method is discussed in detail in the framework of nonlinear principal component analysis and compared to other missing data treatments which are commonly used in this analysis. Its performance vs. these other methods is evaluated throughout a simulation procedure performed on both an artificial case, varying the experimental conditions, and a real case. The proposed procedure is implemented using R.},
  Doi                      = {10.1016/j.csda.2011.02.007},
  Keywords                 = {composite indicators; forward imputation; imputation procedure; listwise deletion; nearest neighbor; ordinal data; passive treatment},
  Owner                    = {nathalie},
  Timestamp                = {2018.06.07},
  Topics                   = {imputation; kNN; factorial data analysis}
}

@Article{finkbeiner_P1979,
  Title                    = {Estimation for the multiple factor model when data are missing},
  Author                   = {Finkbeiner, C.},
  Journal                  = {Psychometrika},
  Year                     = {1979},
  Number                   = {4},
  Pages                    = {409-420},
  Volume                   = {44},

  Abstract                 = {A maximum likelihood method of estimating the parameters of the multiple factor model when data are missing from the sample is presented. A Monte Carlo study compares the method with 5 heuristic methods of dealing with the problem. The present method shows some advantage in accuracy of estimation over the heuristic methods but is considerably more costly computationally.},
  Doi                      = {10.1007/BF02296204},
  Keywords                 = {factor analysis; missing data},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.11},
  Topics                   = {imputation; ML}
}

@Article{follman_wu_B1995,
  Title                    = {An approximate generalized linear model with random effects for informative missing data},
  Author                   = {Follmann, D. and Wu, M.},
  Journal                  = {Biometrics},
  Year                     = {1995},
  Number                   = {1},
  Pages                    = {151-168},
  Volume                   = {51},

  Abstract                 = {This paper develops a class of models to deal with missing data from longitudinal studies. We assume that separate models for the primary response and missingness (e.g., number of missed visits) are linked by a common random parameter. Such models have been developed in the econometrics (Heckman, 1979, Econometrica 47, 153-161) and biostatistics (Wu and Carroll, 1988, Biometrics 44, 175-188) literature for a Gaussian primary response. We allow the primary response, conditional on the random parameter, to follow a generalized linear model and approximate the generalized linear model by conditioning on the data that describes missingness. The resultant approximation is a mixed generalized linear model with possibly heterogeneous random effects. An example is given to illustrate the approximate approach, and simulations are performed to critique the adequacy of the approximation for repeated binary data.},
  Doi                      = {10.2307/2533322},
  ISSN                     = {0006341X, 15410420},
  Owner                    = {alyssa},
  Publisher                = {[Wiley, International Biometric Society]},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar}
}

@Article{gad_darwish_AJAMS2013,
  Title                    = {A shared parameter model for longitudinal data with missing values},
  Author                   = {Gad, A.M. and Darwish, N.M.M.},
  Journal                  = {American Journal of Applied Mathematics and Statistics},
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {30-35},
  Volume                   = {1},

  Abstract                 = {Longitudinal studies represent one of the principal research strategies employed in medical and social research. These studies are the most appropriate for studying individual change over time. The prematurely withdrawal of some subjects from the study (dropout) is termed nonrandom when the probability of missingness depends on the missing value. Nonrandom dropout is common phenomenon associated with longitudinal data and it complicates statistical inference. The shared parameter model is used to fit longitudinal data in the presence of nonrandom dropout. The stochastic EM algorithm is developed to obtain the model parameter estimates. Also, parameter estimates of the dropout model have been obtained. Standard errors of estimates have been calculated using the developed Monte Carlo method. The proposed approach performance is evaluated through a simulation study. Also, the proposed approach is applied to a real data set.},
  Owner                    = {alyssa},
  Timestamp                = {2017.08.07},
  Topics                   = {mnar},
  Url                      = {http://pubs.sciepub.com/ajams/1/2/3}
}

@Article{graham_ARP2009,
  Title                    = {Missing data analysis: making it work in the real world},
  Author                   = {Graham, J.W.},
  Journal                  = {Annual Review of Psychology},
  Year                     = {2009},
  Pages                    = {549-576},
  Volume                   = {60},

  Abstract                 = {This review presents a practical summary of the missing data literature, including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. Practical missing data analysis issues are discussed, most notably the inclusion of auxiliary variables for improving power and reducing bias. Solutions are given for missing data challenges such as handling longitudinal, categorical, and clustered data with normal-model MI; including interactions in the missing data model; and handling large numbers of variables. The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. Strategies suggested for reducing attrition bias include using auxiliary variables, collecting follow-up data on a sample of those initially missing, and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.},
  Doi                      = {10.1146/annurev.psych.58.110405.085530},
  ISBN                     = {0066-4308 (Print) 0066-4308 (Linking)},
  ISSN                     = {0066-4308},
  Mendeley-groups          = {missing data},
  Owner                    = {alyssa},
  Pmid                     = {18652544},
  Shorttitle               = {Missing Data Analysis},
  Timestamp                = {2016.11.30},
  Topics                   = {general}
}

@InBook{graham_etal_SP1997,
  Title                    = {The Science of Prevention: Methodological Advances from Alcohol and Substance Abuse Research},
  Author                   = {Graham, J.W. and Hofer, S.M. and Donaldson, S.I. and MacKinnon, D.P. and Schafer, J.L.},
  Chapter                  = {Analysis with missing data in prevention research},
  Editor                   = {Bryant, K.J. and Windle, M. and West, S.G.},
  Pages                    = {325-366},
  Publisher                = {American Psychological Association},
  Year                     = {1997},

  Address                  = {Washington, DC, USA},

  Abstract                 = {(from the chapter) Outlines leading approaches to dealing with missing data problems, specifically as they apply to alcohol and drug prevention research. First, the authors discuss methods for missing continuous data. (the expectation-maximization algorithm, multiple imputation, multiple-group structural equation modeling, and raw maximum-likelihood). Next, they discuss missing categorical data, present the beginnings of a maximum-likelihood approach to analysis with missing categorical data, discuss the use of a multiple imputation procedure for categorical data, and touch on the use of continuous-data methods for analyzing categorical data. The authors then discuss what happens when the assumptions underlying their recommended approach are not met fully with new data being presented relating to the causes of missingness. A general sensitivity analysis for the case in which the assumptions of the recommended missing data procedures are not met fully is presented. Finally, the authors discuss new approaches available to prevention and applied psychological researchers and suggest that prevention studies in general may be relatively free from serious attrition biases (if recommended analyses are used). (PsycINFO Database Record (c) 2002 APA, all rights reserved).},
  Doi                      = {10.1037/10222-010},
  ISBN                     = {1-55798-439-5},
  ISSN                     = {1046-9516},
  Keywords                 = {alcohol abuse; drug abuse prevention; experimentation; statistical estimation; maximum likelihood},
  Owner                    = {nathalie},
  Pmid                     = {9243532},
  Timestamp                = {2018.07.12},
  Topics                   = {general}
}

@Article{graham_etal_PS2007,
  Title                    = {How many imputations are really needed? Some practical clarifications of multiple imputation theory},
  Author                   = {Graham, John W. and Olchowski, Allison E. and Gilreath, Tamika E.},
  Journal                  = {Prevention Science},
  Year                     = {2007},
  Number                   = {3},
  Pages                    = {206-213},
  Volume                   = {8},

  Abstract                 = {Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information (gamma) for the parameter being estimated, and m. In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which gamma and m were varied. Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. These recommendations are based on gamma, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations.},
  Doi                      = {10.1007/s11121-007-0070-9},
  Keywords                 = {multiple imputation; number of imputations; full information maximum likelihood; missing data; statistical power},
  Owner                    = {nathalie},
  Timestamp                = {2018.06.06},
  Topics                   = {multiple imputation; ML}
}

@Article{heckman_E1979,
  Title                    = {Sample selection bias as a specification error},
  Author                   = {Heckman, J.},
  Journal                  = {Econometrica},
  Year                     = {1979},
  Number                   = {1},
  Pages                    = {153-161},
  Volume                   = {47},

  Abstract                 = {This paper discusses the bias that results from using nonrandomly selected samples to estimate behavioral relationships as an ordinary specification error or "omitted variables" bias. A simple consistent two stage estimator is considered that enables analysts to utilize simple regression methods to estimate behavioral functions by least squares methods. The asymptotic distribution of the estimator is derived.},
  Doi                      = {10.2307/1912352},
  ISSN                     = {00129682, 14680262},
  Owner                    = {alyssa},
  Publisher                = {[Wiley, Econometric Society]},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar}
}

@Article{heckman_AESM1976,
  Title                    = {The common structure of statistical models of truncation, sample selection and limited dependent variables and a simple estimator for such models},
  Author                   = {Heckman, J.J.},
  Journal                  = {Annals of Economic and Social Measurement},
  Year                     = {1976},
  Number                   = {4},
  Pages                    = {475-492},
  Volume                   = {5},

  ISBN                     = {0691003637},
  ISSN                     = {0361-8595},
  Mendeley-groups          = {missing data},
  Owner                    = {alyssa},
  Pmid                     = {9590},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar},
  Url                      = {http://ideas.repec.org/h/nbr/nberch/10491.html}
}

@Article{hogan_laird_SM1997,
  Title                    = {Mixture models for the joint distribution of repeated measures and event times},
  Author                   = {Hogan, J.W. and Laird, N.M.},
  Journal                  = {Statistics in Medecine},
  Year                     = {1997},
  Number                   = {1-3},
  Pages                    = {239-257},
  Volume                   = {16},

  Abstract                 = {Many long-term clinical trials collect both a vector of repeated measurements and an event time on each subject; often, the two outcomes are dependent. One example is the use of surrogate markers to predict disease onset or survival. Another is longitudinal trials which have outcome-related dropout. We describe a mixture model for the joint distribution which accommodates incomplete repeated measures and right-censored event times, and provide methods for full maximum likelihood estimation. The methods are illustrated through analysis of data from a clinical trial for a new schizophrenia therapy; in the trial, dropout time is closely related to outcome, and the dropout process differs between treatments. The parameter estimates from the model are used to make a treatment comparison after adjusting for the effects of dropout. An added benefit of the analysis is that it permits using the repeated measures to increase efficiency of estimates of the event time distribution.},
  Doi                      = {10.1002/(SICI)1097-0258(19970215)16:3<239::AID-SIM483>3.0.CO;2-X},
  Owner                    = {alyssa},
  Timestamp                = {2017.08.07},
  Topics                   = {mnar}
}

@Article{honaker_etal_JSS2011,
  Title                    = {Amelia {II}: a program for missing data},
  Author                   = {Honaker, J. and King, G. and Blackwell, M.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2011},
  Number                   = {7},
  Volume                   = {45},

  Abstract                 = {Amelia II "multiply imputes" missing data in a single cross-section (such as a survey), from a time series (like variables collected for each year in a country), or from a time-series-cross-sectional data set (such as collected by years for each of several countries). Amelia II implements our bootstrapping-based algorithm that gives essentially the same answers as the standard IP or EMis approaches, is usually considerably faster than existing approaches and can handle many more variables. Unlike Amelia I and other statistically rigorous imputation software, it virtually never crashes (but please let us know if you find to the contrary!). The program also generalizes existing approaches by allowing for trends in time series across observations within a cross-sectional unit, as well as priors that allow experts to incorporate beliefs they have about the values of missing cells in their data. Amelia II also includes useful diagnostics of the fit of multiple imputation models. The program works from the R command line or via a graphical user interface that does not require users to know R.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1501.0228},
  Doi                      = {10.18637/jss.v045.i07},
  Eprint                   = {arXiv:1501.0228},
  ISBN                     = {1548-7660},
  ISSN                     = {15487660},
  Owner                    = {alyssa},
  Pmid                     = {18291371},
  Timestamp                = {2017.10.16},
  Topics                   = {multiple imputation; ML}
}

@Article{huisman_QQ2000,
  Title                    = {Imputation of missing item responses: some simple techniques},
  Author                   = {Huisman, M.},
  Journal                  = {Quality \& Quantity},
  Year                     = {2000},
  Number                   = {4},
  Pages                    = {331-351},
  Volume                   = {34},

  Abstract                 = {Among the wide variety of procedures to handle missing data, imputing the missing values is a popular strategy to deal with missing item responses. In this paper some simple and easily implemented imputation techniques like item and person mean substitution, and somehot-deck procedures, are investigated. A simulation study was performed based on responses to items forming a scale to measure a latent trait of the respondents. The effects of different imputation procedures on the estimation of the latent ability of the respondents were investigated, as well as the effect on the estimation of Cronbach's alpha (indicating the reliability of the test) and Loevinger's H-coefficient (indicating scalability). The results indicate that procedures which use the relationships between items perform best, although they tend to overestimate the scale quality.},
  Doi                      = {10.1023/A:1004782230065},
  Keywords                 = {missing data; mean imputation; hot-deck imputation; item response theory; simulation},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.16},
  Topics                   = {imputation; hot-deck}
}

@Article{husson_josse_FQP2013,
  Title                    = {Handling missing values in multiple factor analysis},
  Author                   = {Husson, F. and Josse, J.},
  Journal                  = {Food Quality and Preference},
  Year                     = {2013},
  Pages                    = {77-85},
  Volume                   = {30},

  Doi                      = {10.1016/j.foodqual.2013.04.013},
  Keywords                 = {exploratory multivariate analysis, missing values, multi-table data, multiple factor analysis, napping data},
  Owner                    = {nathalie},
  Timestamp                = {2016.09.28},
  Topics                   = {factorial data analysis; imputation}
}

@Article{ilin_raiko_JMLR2010,
  Title                    = {Practical approaches to {P}rincipal {C}omponent {A}nalysis in the presence of missing values},
  Author                   = {Ilin, A. and Raiko, T.},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2010},
  Pages                    = {1957-2000},
  Volume                   = {11},

  Abstract                 = {Principal component analysis (PCA) is a classical data analysis technique that finds linear transformations of data that retain the maximal amount of variance. We study a case where some of the data values are missing, and show that this problem has many features which are usually associated with nonlinear models, such as overfitting and bad locally optimal solutions. A probabilistic formulation of PCA provides a good foundation for handling missing values, and we provide formulas for doing that. In case of high dimensional and very sparse data, overfitting becomes a severe problem and traditional algorithms for PCA are very slow. We introduce a novel fast algorithm and extend it to variational Bayesian learning. Different versions of PCA are compared in artificial experiments, demonstrating the effects of regularization and modeling of posterior variance. The scalability of the proposed algorithm is demonstrated by applying it to the Netflix problem.},
  ISBN                     = {1532-4435},
  Keywords                 = {missing values; overfitting; principal component analysis; regularization; variational},
  Owner                    = {alyssa},
  Timestamp                = {2016.11.30},
  Topics                   = {factorial data analysis; imputation},
  Url                      = {http://jmlr.csail.mit.edu/papers/v11/ilin10a.html}
}

@Article{imbert_etal_B2018,
  Title                    = {Multiple hot-deck imputation for network inference from {RNA} sequencing data},
  Author                   = {Imbert, A. and Valsesia, A. and Le Gall, C. and Armenise, C. and Lefebvre, G. and Gourraud, P. and Viguerie, N. and Villa-Vialaneix, N.},
  Journal                  = {Bioinformatics},
  Year                     = {2018},
  Number                   = {10},
  Pages                    = {1726-1732},
  Volume                   = {34},

  Abstract                 = {Motivation: Network inference provides a global view of the relations existing between gene expression in a given transcriptomic experiment (often only for a restricted list of chosen genes). However, it is still a challenging problem: even if the cost of sequencing techniques has decreased over the last years, the number of samples in a given experiment is still (very) small compared to the number of genes. Results: We propose a method to increase the reliability of the inference when RNA-seq expression data have been measured together with an auxiliary dataset that can provide external information on gene expression similarity between samples. Our statistical approach, hd-MI, is based on imputation for samples without available RNA-seq data that are considered as missing data but are observed on the secondary dataset. hd-MI can improve the reliability of the inference for missing rates up to 30% and provides more stable networks with a smaller number of false positive edges. On a biological point of view, hd-MI was also found relevant to infer networks from RNA-seq data acquired in adipose tissue during a nutritional intervention in obese individuals. In these networks, novel links between genes were highlighted, as well as an improved comparability between the two steps of the nutritional intervention. Availability: Software and sample data are available as an R package, RNAseqNet, that can be downloaded from the Comprehensive R Archive Network (CRAN).},
  Doi                      = {10.1093/bioinformatics/btx819},
  Owner                    = {nathalie},
  Timestamp                = {2017.07.07},
  Topics                   = {multiple imputation; hot-deck},
  Website                  = {https://academic.oup.com/bioinformatic}
}

@InProceedings{jonsson_wohlin_ISSM2004,
  Title                    = {An evaluation of k-nearest neighbour imputation using lIkert data},
  Author                   = {J{\"{o}}nsson, P. and Wohlin, C.},
  Booktitle                = {Proceedings of the 10th International Symposium on Software Metrics},
  Year                     = {2004},

  Address                  = {Chicago, IL, USA},
  Pages                    = {1530-1435},
  Publisher                = {IEEE},

  Abstract                 = {Studies in many different fields of research suffer from the problem of missing data. With missing data, statistical tests will lose power, results may be biased, or analysis may not be feasible at all. There are several ways to handle the problem, for example through imputation. With imputation, missing values are replaced with estimated values according to an imputation method or model. In the k-nearest neighbour (k-NN) method, a case is imputed using values from the k most similar cases. In this paper, we present an evaluation of the k-NN method using Likert data in a software engineering context. We simulate the method with different values of k and for different percentages of missing data. Our findings indicate that it is feasible to use the k-NN method with Likert data. We suggest that a suitable value of k is approximately the square root of the number of complete cases. We also show that by relaxing the method rules with respect to selecting neighbours, the ability of the method remains high for large amounts of missing data without affecting the quality of the imputation.},
  Doi                      = {10.1109/METRIC.2004.1357895},
  Eventdate                = {2004-09-14/2004-09-16},
  ISBN                     = {0769521290},
  ISSN                     = {15301435},
  Owner                    = {alyssa},
  Timestamp                = {2017.05.29},
  Topics                   = {kNN; imputation}
}

@Article{jamshidian_jalal_P2010,
  Title                    = {Tests of homoscedasticity, normality, and missing completely at random for incomplete multivariate data},
  Author                   = {Jamshidian, M. and Jalal, S.},
  Journal                  = {Psychometrika},
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {649-674},
  Volume                   = {75},

  Abstract                 = {Test of homogeneity of covariances (or homoscedasticity) among several groups has many applications in statistical analysis. In the context of incomplete data analysis, tests of homoscedasticity among groups of cases with identical missing data patterns have been proposed to test whether data are missing completely at random (MCAR). These tests of MCAR require large sample sizes n and/or large group sample sizes n(i), and they usually fail when applied to non-normal data. Hawkins (1981) proposed a test of multivariate normality and homoscedasticity that is an exact test for complete data when n(i) are small. This paper proposes a modification of this test for complete data to improve its performance, and extends its application to test of homoscedasticity and MCAR when data are multivariate normal and incomplete. Moreover, it is shown that the statistic used in the Hawkins test in conjunction with a nonparametric k-sample test can be used to obtain a nonparametric test of homoscedasticity that works well for both normal and non-normal data. It is explained how a combination of the proposed normal-theory Hawkins test and the nonparametric test can be employed to test for homoscedasticity, MCAR, and multivariate normality. Simulation studies show that the newly proposed tests generally outperform their existing competitors in terms of Type I error rejection rates. Also, a power study of the proposed tests indicates good power. The proposed methods use appropriate missing data imputations to impute missing data. Methods of multiple imputation are described and one of the methods is employed to confirm the result of our single imputation methods. Examples are provided where multiple imputation enables one to identify a group or groups whose covariance matrices differ from the majority of other groups.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {NIHMS150003},
  Doi                      = {10.1007/s11336-010-9175-3},
  Eprint                   = {NIHMS150003},
  ISBN                     = {1133601091753},
  ISSN                     = {00333123},
  Keywords                 = {covariance structures,k-sample test,missing data,multiple imputation,nonparametric test,structural equations,test of homogeneity of covariances},
  Owner                    = {alyssa},
  Pmid                     = {21720450},
  Timestamp                = {2017.05.09},
  Topics                   = {diagnosis}
}

@Article{jamshidian_etal_JSS2014,
  Title                    = {{MissMech}: an {R} package for testing homoscedasticity, multivariate normality, and missing completely at random ({MCAR})},
  Author                   = {Jamshidian, M. and Jalal, S. and Jansen, C.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {1-31},
  Volume                   = {56},

  Abstract                 = {Researchers are often faced with analyzing data sets that are not complete. To prop- erly analyze such data sets requires the knowledge of the missing data mechanism. If data are missing completely at random (MCAR), then many missing data analysis techniques lead to valid inference. Thus, tests of MCAR are desirable. The package MissMech im- plements two tests developed by Jamshidian and Jalal (2010) for this purpose. These tests can be run using a function called TestMCARNormality. One of the tests is valid if data are normally distributed, and another test does not require any distributional as- sumptions for the data. In addition to testing MCAR, in some special cases, the function TestMCARNormality is also able to test whether data have a multivariate normal distribu- tion. As a bonus, the functions in MissMech can also be used for the following additional tasks: (i) test of homoscedasticity for several groups when data are completely observed, (ii) perform the k-sample test of Anderson-Darling to determine whether k groups of univariate data come from the same distribution, (iii) impute incomplete data sets using two methods, one where normality is assumed and one where no specific distributional assumptions are made, (iv) obtain normal-theory maximum likelihood estimates for mean and covariance matrix when data are incomplete, along with their standard errors, and finally (v) perform the Neyman's test of uniformity. All of these features are explained in the paper, including examples.},
  Doi                      = {10.18637/jss.v056.i06},
  ISBN                     = {1548-7660},
  ISSN                     = {1548-7660},
  Keywords                 = {anderson-darling,goodness of fit test,hawkins,homogeneity of covariances,im-,incomplete data,maximum likelihood estimate,missing data,neyman,putation,s test,test},
  Owner                    = {alyssa},
  Timestamp                = {2017.05.09},
  Topics                   = {diagnosis}
}

@Article{joenssen_bankhofer_JTACS2012,
  Title                    = {Donor limited hot deck imputation: effect on parameter estimation},
  Author                   = {Joenssen, D.W. and Bankhofer, U.},
  Journal                  = {Journal of Theoretical and Applied Computer Science},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {58-70},
  Volume                   = {6},

  Abstract                 = {Methods for dealing with missing data in the context of large surveys or data mining projects are limited by the computational complexity that they may exhibit. Hot deck imputation methods are computationally simple, yet effective for creating complete data sets from which correct inferences may be drawn. All hot deck methods draw values for the imputation of missing values from the data matrix that will later be analyzed. The object, from which these available values are taken for imputation within another, is called the donor. This duplication of values may lead to the problem that using any donor ``too often'' will induce incorrect estimates. To mitigate this dilemma some hot deck methods limit the amount of times any one donor may be selected. This study answers which conditions influence whether or not any such limitation is sensible for six different hot deck methods. In addition, five factors that influence the strength of any such advantage are identified and possibilities for further research are discussed.},
  Keywords                 = {hot deck imputation, missing data, non-response, imputation, simulation},
  Owner                    = {aimbert},
  Timestamp                = {2017.02.21},
  Topics                   = {imputation; hot-deck}
}

@Article{josse_etal_JC2012,
  Title                    = {Handling missing values with regularized iterative multiple correspondance analysis},
  Author                   = {Josse, Julie and Chavent, Marie and Liquet, Benoi and Husson, Fran\c{c}ois},
  Journal                  = {Journal of Classification},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {91-116},
  Volume                   = {29},

  Abstract                 = {A common approach to deal with missing values in multivariate exploratory data analysis consists in minimizing the loss function over all non-missing elements, which can be achieved by EM-type algorithms where an iterative imputation of the missing values is performed during the estimation of the axes and components. This paper proposes such an algorithm, named iterative multiple correspondence analysis, to handle missing values in multiple correspondence analysis (MCA). The algorithm, based on an iterative PCA algorithm, is described and its properties are studied. We point out the overfitting problem and propose a regularized version of the algorithm to overcome this major issue. Finally, performances of the regularized iterative MCA algorithm (implemented in the R-package named missMDA) are assessed from both simulations and a real dataset. Results are promising with respect to other methods such as the missing-data passive modified margin method, an adaptation of the missing passive method used in Gifi’s Homogeneity analysis framework.},
  Doi                      = {10.1007/s00357-012-9097-0},
  Keywords                 = {multiple correspondence analysis; categorical data; missing values; imputation; regularization},
  Owner                    = {alyssa},
  Timestamp                = {2017.02.22},
  Topics                   = {factorial data analysis; imputation}
}

@Article{josse_husson_JSS2016,
  Title                    = {{missMDA}: a package for handling missing values in multivariate data analysis},
  Author                   = {Josse, J. and Husson, F.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {1-31},
  Volume                   = {70},

  Doi                      = {10.18637/jss.v070.i01},
  Owner                    = {nathalie},
  Timestamp                = {2016.10.17},
  Topics                   = {multiple imputation; factorial data analysis}
}

@Article{josse_husson_JSFdS2012,
  Title                    = {Handling missing values in exploratory multivariate data analysis methods},
  Author                   = {Josse, J. and Husson, F.},
  Journal                  = {Journal de la Soci\'et\'e Fran\c{c}aise de Statistique},
  Year                     = {2012},
  Number                   = {2},
  Pages                    = {79-99},
  Volume                   = {153},

  Abstract                 = {This paper is a written version of the talk Julie Josse delivered at the 44 Journ{\'{e}}es de Statistique (Bruxelles, 2012), when being awarded the Marie-Jeanne Laurent-Duhamel prize for her Ph.D. dissertation by the French Statistical Society. It proposes an overview of some results, proposed in Julie Josse and Fran{\c{c}}ois Husson's papers, as well as new challenges in the field of handling missing values in exploratory multivariate data analysis methods and especially in principal component analysis (PCA). First we describe a regularized iterative PCA algorithm to provide point estimates of the principal axes and components and to overcome the major issue of overfitting. Then, we give insight in the parameters variance using a non parametric multiple imputation procedure. Finally, we discuss the problem of the choice of the number of dimensions and we detail cross-validation approximation criteria. The proposed methodology is implemented in the R package missMDA. R{\'{e}}sum{\'{e}} : Cet article fait suite {\`{a}} la conf{\'{e}}rence de Julie Josse sur ses travaux de th{\`{e}}se lors de la r{\'{e}}ception du prix Marie-Jeanne Laurent-Duhamel, dans le cadre des 44e Journ{\'{e}}es de Statistique (Bruxelles, 2012). Il reprend les principaux r{\'{e}}sultats des papiers de Julie Josse et Fran{\c{c}}ois Husson sur la gestion des donn{\'{e}}es manquantes en analyse factorielle et d{\'{e}}crit de nouvelles avanc{\'{e}}es sur le sujet. Dans un premier temps, nous d{\'{e}}taillons un algorithme d'ACP it{\'{e}}rative r{\'{e}}gularis{\'{e}}e qui permet d'estimer les axes et composantes principales en pr{\'{e}}sence de donn{\'{e}}es manquantes et qui pallie le probl{\`{e}}me majeur du surajustement. L'estimation ponctuelle est enrichie par la construction de zone de confiance. Une m{\'{e}}thode d'imputation multiple non-param{\'{e}}trique est alors d{\'{e}}velopp{\'{e}}e pour prendre en compte l'incertitude due aux donn{\'{e}}es manquantes. Enfin, nous abordons le probl{\`{e}}me r{\'{e}}current du choix du nombre de dimensions et d{\'{e}}finissons des approximations de la validation crois{\'{e}}e de type validation crois{\'{e}}e g{\'{e}}n{\'{e}}ralis{\'{e}}e. Tous ces travaux sont mis {\`{a}} disposition de l'utilisateur gr{\^{a}}ce au package missMDA du logiciel libre R.},
  ISSN                     = {2102-6238},
  Owner                    = {alyssa},
  Timestamp                = {2016.09.27},
  Topics                   = {multiple imputation; factorial data analysis},
  Url                      = {http://publications-sfds.fr/ojs/index.php/J-SFdS/article/view/122/112}
}

@Article{josse_etal_JSFdS2009,
  Title                    = {Gestion des donn\'ees manquantes en {A}nalyse en {C}omposantes {P}rincipales},
  Author                   = {Josse, J. and Husson, F. and Pag\`es, J.},
  Journal                  = {Journal de la Soci\'et\'e Fran\c{c}aise de Statistique},
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {28-51},
  Volume                   = {150},

  Abstract                 = {An approach commonly used to handle missing values in Principal Component Analysis (PCA) consists in ignoring the missing values by optimizing the loss function over all non-missing ele- ments. This can be achieved by several methods, including the use of NIPALS, weighted regression or iterative PCA. The latter is based on iterative imputation of the missing elements during the es- timation of the parameters, and can be seen as a particular EM algorithm. First, we review theses approaches with respect to the criterion minimization. This presentation gives a good understanding of their properties and the difficulties encountered. Then, we point out the problem of overfitting and we show how the probabilistic formulation of PCA (Tipping {\&} Bishop, 1997) offers a proper and convenient regularization term to overcome this problem. Finally, the performances of the new algorithm are compared to those of the other algorithms from simulations.},
  Keywords                 = {ACP; ACP probabiliste; ACP-GEM; algorithme EM; donn\'ees manquantes; moindres carr\'es altern\'es pond\'er\'es; surajustement},
  Owner                    = {alyssa},
  Timestamp                = {2016.11.30},
  Topics                   = {factorial data analysis; imputation},
  Url                      = {http://journal-sfds.fr/ojs/index.php/J-SFdS/article/view/33/27}
}

@Article{josse_etal_ADAC2011,
  Title                    = {Multiple imputation in principal component analysis},
  Author                   = {Josse, J. and Pag\`es, J. and Husson, F.},
  Journal                  = {Advances in Data Analysis and Classification},
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {231-246},
  Volume                   = {5},

  Abstract                 = {The available methods to handle missing values in principal component analysis only provide point estimates of the parameters (axes and components) and estimates of the missing values. To take into account the variability due to missing values a multiple imputation method is proposed. First a method to generate multiple imputed data sets from a principal component analysis model is defined. Then, two ways to visualize the uncertainty due to missing values onto the principal component analysis results are described. The first one consists in projecting the imputed data sets onto a reference configuration as supplementary elements to assess the stability of the individuals (respectively of the variables). The second one consists in performing a principal component analysis on each imputed data set and fitting each obtained configuration onto the reference one with Procrustes rotation. The latter strategy allows to assess the variability of the principal component analysis parameters induced by the missing values. The methodology is then evaluated from a real data set.},
  Doi                      = {10.1007/s11634-011-0086-7},
  Keywords                 = {Bootstrap;EM algorithm;Missing values;Multiple imputation;Principal component analysis;Procrustes rotation},
  Owner                    = {alyssa},
  Timestamp                = {2016.12.20},
  Topics                   = {multiple imputation; factorial data analysis}
}

@Article{kaiser_JSI2014,
  Title                    = {Dealing with missing values in data},
  Author                   = {Kaiser, J.},
  Journal                  = {Journal of Systems Integration},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {42-51},
  Volume                   = {5},

  Abstract                 = {Many existing industrial and research data sets contain missing values due to various reasons, such as manual data entry procedures, equipment errors and incorrect measurements. Problems associated with missing values are loss of efficiency, complications in handling and analyzing the data and bias resulting from differences between missing and complete data. The important factor for selection of approach to missing values is missing data mechanism. There are various strategies for dealing with missing values. Some analytical methods have their own approach to handle missing values. Data set reduction is another option. Finally missing values problem can be handled by missing values imputation. This paper presents simple methods for missing values imputation like using most common value, mean or median, closest fit approach and methods based on data mining algorithms like k-nearest neighbor, neural networks and association rules, discusses their usability and presents issues with their applicability on examples.},
  Doi                      = {10.20470/jsi.v5i1.178},
  ISBN                     = {18042724},
  ISSN                     = {18042724},
  Owner                    = {alyssa},
  Pmid                     = {94265105},
  Timestamp                = {2017.05.29},
  Topics                   = {general}
}

@Article{kalton_kasprzyk_SM1986,
  Title                    = {The treatment of missing survey data},
  Author                   = {Kalton, G. and Kasprzyk, D.},
  Journal                  = {Survey Methodology},
  Year                     = {1986},
  Number                   = {1},
  Pages                    = {1-16},
  Volume                   = {12},

  Abstract                 = {Missing survey data occur because of total nonresponse and item nonresponse. The standard way to attempt to compensate for total nonresponse is by some form of weighting adjustment, whereas item nonresponses are handled by some form of imputation. This paper reviews methods of weighting adjustment and imputation and discusses their properties.},
  Keywords                 = {nonresponse, item nonresponse, weiweight adjustments, imputation},
  Owner                    = {alyssa},
  Timestamp                = {2017.06.07},
  Topics                   = {imputation; ipw},
  Url                      = {http://www.statcan.gc.ca/pub/12-001-x/1986001/article/14404-eng.pdf}
}

@Article{kohn_ansley_JASA1986,
  Title                    = {Estimation, prediction, and interpolation for {ARIMA} models with missing data},
  Author                   = {Kohn, Robert and Ansley, Craig F.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1986},
  Number                   = {395},
  Pages                    = {751-761},
  Volume                   = {81},

  Abstract                 = {We show how to define and then compute efficiently the marginal likelihood of an ARIMA model with missing observations. The computation is carried out by using the univariate version of the modified Kalman filter introduced by Ansley and Kohn (1985a), which allows a partially diffuse initial state vector. We also show how to predict and interpolate missing observations and obtain the mean squared error of the estimate.},
  Doi                      = {10.2307/2289007},
  ISSN                     = {01621459},
  Keywords                 = {Kalman filters; datasets; data smoothing; modeling; missing data; interpolation; state vectors; covariance matrices; maximum likelihood estimation; time series models},
  Owner                    = {alyssa},
  Publisher                = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  Timestamp                = {2018.06.07},
  Topics                   = {time series; imputation}
}

@Article{kowarik_templ_JSS2016,
  Title                    = {Imputation with the {R} Package {VIM}},
  Author                   = {Kowarik, A. and Templ, M.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2016},
  Number                   = {7},
  Pages                    = {1-16},
  Volume                   = {74},

  Abstract                 = {The package VIM (Templ, Alfons, Kowarik, and Prantner 2016) is developed to explore and analyze the structure of missing values in data using visualization methods, to impute these missing values with the built-in imputation methods and to verify the imputation process using visualization tools, as well as to produce high-quality graphics for publications. This article focuses on the different imputation techniques available in the package. Four different imputation methods are currently implemented in VIM, namely hot-deck imputation, k-nearest neighbor imputation, regression imputation and iterative robust model-based imputation (Templ, Kowarik, and Filzmoser 2011). All of these methods are implemented in a flexible manner with many options for customization. Furthermore in this article practical examples are provided to highlight the use of the implemented methods on real-world applications. In addition, the graphical user interface of VIM has been re-implemented from scratch resulting in the package VIMGUI (Schopfhauser, Templ, Alfons, Kowarik, and Prantner 2016) to enable users without extensive R skills to access these imputation and visualization methods.},
  Doi                      = {10.18637/jss.v074.i07},
  Owner                    = {nathalie},
  Timestamp                = {2017.05.29},
  Topics                   = {diagnosis; imputation; general}
}

@Article{little_JASA1995,
  Title                    = {Modeling the drop-out mechanism in repeated-measures studies},
  Author                   = {Little, R.J.A.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1995},
  Number                   = {431},
  Pages                    = {1112-1121},
  Volume                   = {90},

  Abstract                 = {Subjects often drop out of longitudinal studies prematurely, yielding unbalanced data with unequal numbers of measures for each subject. Modern software programs for handling unbalanced longitudinal data improve on methods that discard the incomplete cases by including all the data, but also yield biased inferences under plausible models for the drop-out process. This article discusses methods that simultaneously model the data and the drop-out process within a unified model-based framework. Models are classified into two broad classes--random-coefficient selection models and random-coefficient pattern-mixture models--depending on how the joint distribution of the data and drop-out mechanism is factored. Inference is likelihood-based, via maximum likelihood or Bayesian methods. A number of examples in the literature are placed in this framework, and possible extensions outlined. Data collection on the nature of the drop-out process is advocated to guide the choice of model. In cases where the drop-out mechanism is not well understood, sensitivity analyses are suggested to assess the effect on inferences about target quantities of alternative assumptions about the drop-out process.},
  Doi                      = {10.2307/2291350},
  Keywords                 = {attrition;longitudinal data;missing data;nonrandom nonresponse;selection bias},
  Owner                    = {alyssa},
  Timestamp                = {2016.12.12},
  Topics                   = {mnar}
}

@Article{little_JASA1993,
  Title                    = {Pattern-mixture models for multivariate incomplete data},
  Author                   = {Little, R.J.A.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1993},
  Number                   = {421},
  Pages                    = {125-134},
  Volume                   = {88},

  Abstract                 = {Consider a random sample on variables X1, ..., XV with some values of XV missing. Selection models specify the distribution of X1, ..., XV over respondents and nonrespondents to XV, and the conditional distribution that XV is missing given X1, ..., XV. In contrast, pattern-mixture models specify the conditional distribution of X1, ..., XV given that XV is observed or missing respectively and the marginal distribution of the binary indicator for whether or not XV is missing. For multivariate data with a general pattern of missing values, the literature has tended to adopt the selection-modeling approach (see for example Little and Rubin); here, pattern-mixture models are proposed for this more general problem. Pattern-mixture models are chronically underidentified; in particular for the case of univariate nonresponse mentioned above, there are no data on the distribution of XV given X1, ..., XV-1 in the stratum with XV missing. Thus the models require restrictions or prior information to identify the parameters. Complete-case restrictions tie unidentified parameters to their (identified) analogs in the stratum of complete cases. Alternative types of restriction tie unidentified parameters to parameters in other missing-value patterns or sets of such patterns. This large set of possible identifying restrictions yields a rich class of missing-data models. Unlike ignorable selection models, which generally requires iterative methods except for special missing-data patterns, some pattern-mixture models yield explicit ML estimates for general patterns. Such models are readily amenable to Bayesian methods and form a convenient basis for multiple imputation. Some previously considered noniterative estimation methods are shown to be maximum likelihood (ML) under a pattern-mixture model. For example, Buck's method for continuous data, corrected as in Beale and Little (1975), and Brown's estimators for nonrandomly missing data are ML for pattern-mixture models with particular complete-case restrictions. Available-case analyses, where the mean and variance of Xj are computed using all cases with Xj observed and the correlation (or covariance) of Xj and Xk is computed using all cases with Xj and Xk observed, are also close to ML for another pattern-mixture model. Asymptotic theory for this class of estimators is outlined.},
  Doi                      = {10.2307/2290705},
  ISSN                     = {01621459},
  Keywords                 = {parametric models; statistical estimation; statistical models; missing data; covariance matrices; statistical variance; modeling; mathematical models; data models; sample mean},
  Owner                    = {alyssa},
  Publisher                = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  Timestamp                = {2017.11.14},
  Topics                   = {mnar}
}

@Article{little_JASA1988,
  Title                    = {A test of missing completely at random for multivariate data with missing values},
  Author                   = {Little, R.J.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1988},
  Number                   = {404},
  Pages                    = {1198-1202},
  Volume                   = {83},

  Booktitle                = {Journal of the American Statistical Association},
  Doi                      = {10.2307/2290157},
  ISBN                     = {01621459},
  ISSN                     = {02776715},
  Keywords                 = {datasets; missing data; covariance matrices; sampling distributions; T tests; data sampling; maximum likelihood estimation; standard error; ratio test},
  Owner                    = {alyssa},
  Pmid                     = {9280038},
  Timestamp                = {2018.05.11},
  Topics                   = {diagnosis}
}

@Book{little_rubin_SAMD2002,
  Title                    = {Statistical Analysis with Missing Data},
  Author                   = {Little, R.J.A. and Rubin, D.B.},
  Publisher                = {Wiley},
  Year                     = {2002},

  Booktitle                = {Statistical analysis with missing data Second edition},
  Doi                      = {10.2307/1533221},
  ISBN                     = {0471183865},
  ISSN                     = {00324663},
  Owner                    = {alyssa},
  Pages                    = {408},
  Pmid                     = {10403256},
  Timestamp                = {2016.09.27},
  Topics                   = {general}
}

@Article{little_JASA1992,
  Title                    = {Regression with missing {X}'s: a review},
  Author                   = {Little, Roderick J.A.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1992},
  Number                   = {420},
  Pages                    = {1227-1237},
  Volume                   = {87},

  Abstract                 = {The literature of regression analysis with missing values of the independent variables is reviewed. Six classes of procedures are distinguished: complete case analysis, available case methods, least squares on imputed data, maximum likelihood, Bayesian methods, and multiple imputation. Methods are compared and illustrated when missing data are confined to one independent variable, and extensions to more general patterns are indicated. Attention is paid to the performance of methods when the missing data are not missing completely at random. Least squares methods that fill in missing X's using only data on the X's are contrasted with likelihood-based methods that use data on the X's and Y. The latter approach is preferred and provides methods for elaboration of the basic normal linear regression model. It is suggested that more widely distributed software is needed that advances beyond complete-case analysis, available-case analysis, and naive imputation methods. Bayesian simulation methods and multiple imputation are reviewed; these provide fruitful avenues for future research.},
  Doi                      = {10.2307/2290664},
  ISBN                     = {01621459},
  ISSN                     = {01621459},
  Keywords                 = {bayesian inference; imputation; incomplete data; multiple imputation},
  Owner                    = {alyssa},
  Pmid                     = {318},
  Timestamp                = {2018.06.07},
  Topics                   = {general; ML; multiple imputation}
}

@Article{meng_rubin_B1993,
  Title                    = {Maximum likelihood estimation via the {ECM} algorithm: a general framework},
  Author                   = {Meng, S.L. and Rubin, D.B.},
  Journal                  = {Biometrika},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {267-278},
  Volume                   = {80},

  Abstract                 = {Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computa-tionally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated. We introduce a class of generalized EM algorithms, which we call the ECM algorithm, for Expecta-tion/Conditional Maximization (CM), that takes advantage of the simplicity of complete-data conditional maximum likelihood estimation by replacing a complicated M-step of EM with several computationally simpler cM-steps. We show that the ECM algorithm shares all the appealing convergence properties of EM, such as always increasing the likelihood, and present several illustrative examples.},
  Doi                      = {10.1093/biomet/80.2.267},
  ISBN                     = {00063444},
  ISSN                     = {00063444},
  Keywords                 = {Bayesian inference; conditional maximization; constrained optimization; EM algorithm; Gibbs sampler; incomplete data; iterated conditional modes; iterative proportional fitting; missing data},
  Owner                    = {alyssa},
  Timestamp                = {2017.08.31},
  Topics                   = {ML}
}

@Article{meng_rubin_JASA1991,
  Title                    = {Using {EM} to obtain asymptotic variance-covariance matrices: the {SEM} algorithm},
  Author                   = {Meng, X.L. and Rubin, D.B.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1991},
  Number                   = {416},
  Pages                    = {899-909},
  Volume                   = {86},

  Abstract                 = {The expectation maximization (EM) algorithm estimation in incomplete-data parameters matrices using only the code for computing missing infornation matrix. We call this supplemented that the SEM algorithm tiparameter environments. is a popular, and often remarkably (e.g., standard errors) are not automatic byproducts, to find the increased variability EM algorithm can be a practically problems. One criticism of EM in practice is that asymptotic Raphson. In this article we define and illustrate a procedure that obtains numerically the complete-data important due to missing information the SEM algorithm. supplement SEM can also be used as a tool for monitoring whether simple, method for maximum likelihood variance-covariance matrices for as they are when using some other methods, such as Newton- stable asymptotic standard matrix operations. The basic idea is to use the fact that the rate of convergence of EM is governed by the fractions to add to the complete-data Theory and particular examples reinforce variance-covariance variance-covariance matrix, the code for EM itself, and code for of variance-covariance the conclusion to EM in many problems. SEM is especially useful in mul- problems where only a subset of the parameters are affected by missing infonnation and in parallel computing EM has converged to a (local) maximum.},
  Doi                      = {10.1080/01621459.1991.10475130},
  ISBN                     = {01621459},
  ISSN                     = {1537274X},
  Keywords                 = {Bayesian inference;convergence rate; EM algorithm; incomplete data; maximum likelihood estimation; observed information},
  Owner                    = {alyssa},
  Pmid                     = {298},
  Timestamp                = {2017.10.25},
  Topics                   = {ML}
}

@Article{moeur_stage_FS1995,
  Title                    = {Most similar neighbor: an improved sampling inference procedure for natural resources planning},
  Author                   = {Moeur, M. and Stage, A.R.},
  Journal                  = {Forest Science},
  Year                     = {1995},
  Number                   = {1},
  Pages                    = {337-359},
  Volume                   = {42},

  Owner                    = {nathalie},
  Timestamp                = {2017.10.12},
  Topics                   = {kNN; imputation}
}

@Book{molenberghs_etal_HMDM2014,
  Title                    = {Handbook of Missing Data Methodology},
  Author                   = {Molenberghs, G. and Fitzmaurice, G. and Kenward, M. G. and Tsiatis,A. and Verbeke, G.},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {2014},
  Series                   = {Chapman \& Hall/CRC Handbooks of Modern Statistical Methods},

  Owner                    = {alyssa},
  Timestamp                = {2017.11.14},
  Topics                   = {general}
}

@Article{molenberghs_etal_SN1998,
  Title                    = {Monotone missing data and pattern-mixture models},
  Author                   = {Molenberghs, G. and Michiels, B. and Kenward, M.G. and Diggle, P.J.},
  Journal                  = {Statistica Neerlandica},
  Year                     = {1998},
  Number                   = {2},
  Pages                    = {153-161},
  Volume                   = {52},

  Abstract                 = {It is shown that the classical taxonomy of missing data models, namely missing completely at random, missing at random and informative missingness, which has been developed almost exclusively within a selection modelling framework, can also be applied to pattern-mixture models. In particular, intuitively appealing identifying restrictions are proposed for a pattern-mixture MAR mechanism.},
  Doi                      = {10.1111/1467-9574.00075},
  ISBN                     = {0039-0402},
  ISSN                     = {0039-0402},
  Keywords                 = {missing at random; phrases; selection model},
  Owner                    = {alyssa},
  Timestamp                = {2017.11.14},
  Topics                   = {mnar}
}

@Article{molnar_etal_CMAJ2008,
  Title                    = {Does analysis using ``last observation carried forward'' introduce bias in dementia research?},
  Author                   = {Molnar, F.J. and Hutton, B. and Fergusson, D.},
  Journal                  = {Canadian Medical Association Journal},
  Year                     = {2008},
  Number                   = {8},
  Pages                    = {751-753},
  Volume                   = {179},

  Abstract                 = {If there were a prize for the most inappropriate analytical technique in dementia research, ``last observation carried forward'' would be the runaway winner. As a society, we have spent millions of dollars on drug research in the hope of improving the care of the estimated 24.3 million people who have dementia worldwide. Researchers, patients and families have dedicated countless hours to carrying out trials to test the efficacy of drugs to treat dementia. We then take this invaluable data and, in accordance with US Food and Drug Administration regulation, subject it to last observation carried forward, a form of analysis that introduces bias.},
  Doi                      = {10.1503/cmaj.080820},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.09},
  Topics                   = {time series; imputation}
}

@Article{moritz_bartzbeielstein_RJ2017,
  Title                    = {{imputeTS}: time series missing value imputation in {R}},
  Author                   = {Moritz, Steffen and Bartz-Beielstein, Thomas},
  Journal                  = {The R Journal},
  Year                     = {2017},
  Number                   = {1},
  Pages                    = {207-218},
  Volume                   = {9},

  Abstract                 = {The imputeTS package specializes on univariate time series imputation. It offers multiple state-of-the-art imputation algorithm implementations along with plotting functions for time series missing data statistics. While imputation in general is a well-known problem and widely covered by R packages, finding packages able to fill missing values in univariate time series is more complicated. The reason for this lies in the fact that most imputation algorithms rely on inter-attribute correlations, while univariate time series imputation instead needs to employ time dependencies. This paper provides an introduction to the imputeTS package and its provided algorithms and tools. Furthermore, it gives a short overview about univariate time series imputation in R.},
  ISSN                     = {20734859},
  Owner                    = {alyssa},
  Timestamp                = {2018.06.07},
  Topics                   = {time series; imputation},
  Url                      = {https://journal.r-project.org/archive/2017/RJ-2017-009/index.html}
}

@Unpublished{moritz_etal_p2015,
  Title                    = {Comparison of different methods for univariate time series imputation in {R}},
  Author                   = {Moritz, S. and Sard\'a, A. and Bartz-Beielstein, T. and Zaefferer, M. and Stork, J.},
  Note                     = {Prepint arXiv 1510.03924},
  Year                     = {2015},

  Owner                    = {nathalie},
  Timestamp                = {2017.07.13},
  Topics                   = {imputation; time series},
  Url                      = {https://arxiv.org/abs/1510.03924}
}

@Article{peugh&enders_RER2004,
  Title                    = {Missing data in educational research: a review of reporting practices and suggestions for improvement},
  Author                   = {Peugh, J. L. and Enders, C. K.},
  Journal                  = {Review of Educational Research},
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {525--556},
  Volume                   = {74},

  Abstract                 = {Missing data analyses have received considerable recent attention in the methodological literature, and two “modern” methods, multiple imputation and maximum likelihood estimation, are recommended. The goals of this article are to (a) provide an overview of missing-data theory, maximum likelihood estimation, and multiple imputation; (b) conduct a methodological review of missing-data reporting practices in 23 applied research journals; and (c) provide a demonstration of multiple imputation and maximum likelihood estimation using the Longitudinal Study of American Youth data. The results indicated that explicit discussions of missing data increased substantially between 1999 and 2003, but the use of maximum likelihood estimation or multiple imputation was rare; the studies relied almost exclusively on listwise and pairwise deletion.},
  Doi                      = {10.3102/00346543074004525},
  Owner                    = {alyssa},
  Timestamp                = {2018.07.12},
  Topics                   = {general},
  Url                      = {http://dx.doi.org/10.3102/00346543074004525}
}

@Article{pigott_ERE2001,
  Title                    = {A review of methods for missing data},
  Author                   = {Pigott, T.D.},
  Journal                  = {Educational Research and Evaluation},
  Year                     = {2001},
  Number                   = {4},
  Pages                    = {353-383},
  Volume                   = {7},

  Abstract                 = {This paper reviews methods for handling missing data in a research study. Many researchers use ad hoc methods such as complete case analysis, available case analysis (pairwise deletion), or single-value imputation. Though these methods are easily implemented, they require assumptions about the data that rarely hold in practice. Model-based methods such as maximum likelihood using the EM algorithm and multiple imputation hold more promise for dealing with difficulties caused by missing data. While model-based methods require specialized computer programs and assumptions about the nature of the missing data, these methods are appropriate for a wider range of situations than the more commonly used ad hoc methods. The paper provides an illustration of the methods using data from an intervention study designed to increase students' ability to control their asthma symptoms.},
  Doi                      = {10.1076/edre.7.4.353.8937},
  Owner                    = {alyssa},
  Timestamp                = {2017.02.21},
  Topics                   = {general}
}

@Article{rao_shao_B1992,
  Title                    = {Jackknife variance estimation with survey data under hot deck imputation},
  Author                   = {Rao, J.N.K. and Shao, J.},
  Journal                  = {Biometrika},
  Year                     = {1992},
  Number                   = {4},
  Pages                    = {811-822},
  Volume                   = {79},

  Abstract                 = {Hot deck imputation is commonly employed for item nonresponse in sample surveys. It is also a common practice to treat the imputed values as if they are true values, and then compute the variance estimates using standard formulae. This procedure, however, could lead to serious underestimation of the true variance, when the proportion of missing values for an item is appreciable. We propose a jackknife variance estimator for stratified multistage surveys which is obtained by first adjusting the imputed values for each pseudo-replicate and then applying the standard jackknife formula. The proposed jack-knife variance estimator is shown to be consistent as the sample size increases, assuming equal response probabilities within imputation classes and using a particular hot deck imputation.},
  Doi                      = {10.2307/2337236},
  Owner                    = {nathalie},
  Timestamp                = {2018.06.06},
  Topics                   = {hot-deck; imputation}
}

@Article{reilly_pepe_SM1997,
  Title                    = {The relationship between hot-deck multiple imputation and weighted likelihood},
  Author                   = {Reilly, M. and Pepe, M.},
  Journal                  = {Statistics in Medecine},
  Year                     = {1997},
  Number                   = {1-3},
  Pages                    = {5-19},
  Volume                   = {16},

  Abstract                 = {Hot-deck imputation is an intuitively simple and popular method of accommodating incomplete data. Users of the method will often use the usual multiple imputation variance estimator which is not appropriate in this case. However, no variance expression has yet been derived for this easily implemented method applied to missing covariates in regression models. The simple hot-deck method is in fact asymptotically equivalent to the mean-score method for the estimation of a regression model parameter, so that hot-deck can be understood in the context of likelihood methods. Both of these methods accommodate data where missingness may depend on the observed variables but not on the unobserved value of the incomplete covariate, that is, missing at random (MAR). The asymptotic properties of hot-deck are derived here for the case where the fully observed variables are categorical, though the incomplete covariate(s) may be continuous. Simulation studies indicate that the two methods compare well in small samples and for small numbers of imputations. Current users of hot-deck may now conduct their analysis using mean-score, which is a weighted likelihood method and can thus be implemented by a single pass through the data using any standard package which accommodates weighted regression models. Valid inference is now straightforward using the variance expression provided here. The equivalence of mean-score and hot-deck is illustrated using three clinical data sets where an important covariate is missing for a large number of study subjects.},
  Doi                      = {10.1002/(SICI)1097-0258(19970115)16:1%3C5::AID-SIM469%3E3.0.CO;2-8},
  Owner                    = {nathalie},
  Timestamp                = {2018.04.16},
  Topics                   = {hot-deck; multiple imputation; ML}
}

@Article{robins_etal_JASA1995,
  Title                    = {Analysis of semiparametric regression models for repeated outcomes in the presence of missing data},
  Author                   = {Robins, J.M. and Rotnitzky, A. and Zhao, L.P.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1995},
  Number                   = {429},
  Pages                    = {106-121},
  Volume                   = {90},

  Abstract                 = {We propose a class of inverse probability of censoring weighted estimators for the parameters of models for the dependence of the mean of a vector of correlated response variables on a vector of explanatory variables in the presence of missing response data. The proposed estimators do not require full specification of the likelihood. They can be viewed as an extension of generalized estimating equations estimators that allow for the data to be missing at random but not missing completely at random. These estimators can be used to correct for dependent censoring and nonrandom noncompliance in randomized clinical trials studying the effect of a treatment on the evolution over time of the mean of a response variable. The likelihood-based parametric G-computation algorithm estimator may also be used to attempt to correct for dependent censoring and nonrandom noncompliance. But because of possible model misspecification, the parametric G-computation algorithm estimator, in contrast with the proposed weighted estimators, may be inconsistent for the difference in treatment-arm-specific means, even when compliance is completely at random and censoring is independent. We illustrate our methods with the analysis of the effect of zidovudine (AZT) treatment on the evolution of mean CD4 count with data from an AIDS clinical trial.},
  Doi                      = {10.2307/2291134},
  ISSN                     = {01621459},
  Owner                    = {alyssa},
  Publisher                = {American Statistical Association, Taylor \& Francis},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar}
}

@Article{robins_wang_B2000,
  Title                    = {Inference for imputation estimators},
  Author                   = {Robins, J.M. and Wang, N.},
  Journal                  = {Biometrika},
  Year                     = {2000},
  Number                   = {1},
  Pages                    = {113-124},
  Volume                   = {87},

  Abstract                 = {We derive an estimator of the asymptotic variance of both single and multiple imputation estimators. We assume a parametric imputation model but allow for non- and semiparametric analysis models. Our variance estimator, in contrast to the estimator proposed by Rubin (1987), is consistent even when the imputation and analysis models are misspecified and incompatible with one another.},
  Keywords                 = {estimators; statistical variance; missing data; parametric models; data imputation; consistent estimators; modeling; datasets; analytical estimating; estimation bias},
  Owner                    = {nathalie},
  Timestamp                = {2018.04.16},
  Topics                   = {multiple imputation; ipw},
  Url                      = {https://www.jstor.org/stable/2673565}
}

@Article{rosseel_JSS2012,
  Title                    = {{lavaan}: an {R} package for structural equation modeling},
  Author                   = {Rosseel, Y.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2012},
  Number                   = {2},
  Volume                   = {48},

  Abstract                 = {Structural equation modeling (SEM) is a vast field and widely used by many applied researchers in the social and behavioral sciences. Over the years, many software packages for structural equation modeling have been developed, both free and commercial. However, perhaps the best state-of-the-art software packages in this field are still closed-source and/or commercial. The R package lavaan has been developed to provide applied researchers, teachers, and statisticians, a free, fully open-source, but commercial-quality package for latent variable modeling. This paper explains the aims behind the development of the package, gives an overview of its most important features, and provides some examples to illustrate how lavaan works in practice.},
  Doi                      = {10.18637/jss.v048.i02},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.15},
  Topics                   = {ML}
}

@Article{rotnitzky_etal_JASA1998,
  Title                    = {Semiparametric regression for repeated outcomes with nonignorable nonresponse},
  Author                   = {Rotnitzky, A. and Robins, J.M. and Scharfstein, D.O.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1998},
  Number                   = {444},
  Pages                    = {1321-1339},
  Volume                   = {93},

  Abstract                 = {We consider inference about the parameter beta* indexing the conditional mean of a vector of correlated outcomes given a vector of explanatory variables when some of the outcomes are missing in a subsample of the study and the probability of response depends on both observed and unobserved data values; that is, nonresponse is nonignorable. We propose a class of augmented inverse probability of response weighted estimators that are consistent and asymptotically normal (CAN) for estimating beta* when the response probabilities can be parametrically modeled and a CAN estimator exists. The proposed estimators do not require full specification of a parametric likelihood, and their computation does not require numerical integration. Our estimators can be viewed as an extension of generalized estimating equation estimators that allows for nonignorable nonresponse. We show that our class essentially consists of all CAN estimators of beta*. We also show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model. When the model for nonresponse is richly parameterized, joint estimation of the regression parameter beta* and the nonresponse model parameter tau* which encodes the magnitude of nonignorable selection bias, may be difficult or impossible. Therefore we propose regarding the selection bias parameter tau* as known, rather than estimating it from the data. We then perform a sensitivity analysis that examines how inference concerning the regression parameter beta* changes as we vary tau* over a range of plausible values. We apply our approach to the analysis of ACTG Trial 002, an AIDS clinical trial.},
  Doi                      = {10.2307/2670049},
  ISSN                     = {01621459},
  Keywords                 = {curse of dimensionality; estimating equations; identification; missing data; semiparametric efficiency; sensitivity analysis},
  Owner                    = {alyssa},
  Publisher                = {American Statistical Association, Taylor \& Francis},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar}
}

@Article{rubin_JASA2012,
  Title                    = {Multiple imputation after 18+ years},
  Author                   = {Rubin, D.B.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2012},
  Number                   = {434},
  Pages                    = {473-489},
  Volume                   = {91},

  Abstract                 = {Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies},
  Doi                      = {10.1080/01621459.1996.10476908},
  ISBN                     = {0162-1459},
  ISSN                     = {0162-1459},
  Keywords                 = {confidence validity,missing data,nonresponse in surveys,public-use files,sample surveys,superefficient procedures},
  Owner                    = {alyssa},
  Timestamp                = {2016.09.27},
  Topics                   = {multiple imputation}
}

@Book{rubin_MINS1987,
  Title                    = {Multlipe Imputation for Nonresponse in Surveys},
  Author                   = {Rubin, D.B.},
  Publisher                = {Wiley},
  Year                     = {1987},

  Owner                    = {alyssa},
  Timestamp                = {2016.09.27},
  Topics                   = {multiple imputation}
}

@Article{rubin_JASA1977,
  Title                    = {Formalizing subjective notions about the effect of nonrespondents in sample surveys},
  Author                   = {Rubin, D.B.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1977},
  Number                   = {359},
  Pages                    = {538-543},
  Volume                   = {72},

  Abstract                 = {A method is given for estimating, in a subjective sense, the effect of nonresponse in sample surveys. Based on Bayesian techniques, this method produces a subjective probability interval for the statistic that would have been calculated if all nonrespondents had responded. Background information which is recorded for both respondents and nonrespondents plays an important role in sharpening the subjective interval. Real survey data of 660 schools with 188 nonrespondents indicates that the method can be useful in practical problems. The general idea can be applied to any problem with nonrespondents or missing data.},
  Doi                      = {10.2307/2286214},
  ISSN                     = {01621459},
  Owner                    = {alyssa},
  Publisher                = {American Statistical Association, Taylor \& Francis},
  Timestamp                = {2016.12.12},
  Topics                   = {ML}
}

@Article{rubin_B1976,
  Title                    = {Inference and missing data},
  Author                   = {Rubin, D.B.},
  Journal                  = {Biometrika},
  Year                     = {1976},
  Number                   = {3},
  Pages                    = {581-592},
  Volume                   = {63},

  Abstract                 = {When making sampling distribution inferences about the parameter of the data, theta, it is appropriate to ignore the process that causes missing data if the missing data are "missing at random" and the observed data are "observed at random", but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about theta, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is "distinct" from theta. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
  Doi                      = {10.1093/biomet/63.3.581},
  Keywords                 = {Bayesian inference; incomplete data; likelikhood inference; missing at random; missing data; observed at random; sampling distribution inference},
  Owner                    = {nathalie},
  Timestamp                = {2018.04.16},
  Topics                   = {ML}
}

@Article{schafer_SMMR1999,
  Title                    = {Multiple imputation: a primer},
  Author                   = {Schafer, J.L.},
  Journal                  = {Statistical Methods in Medical Research},
  Year                     = {1999},
  Number                   = {1},
  Pages                    = {3-15},
  Volume                   = {8},

  Abstract                 = {In recent years, multiple imputation has emerged as a convenient and flexible paradigm for analysing data with missing values. Essential features of multiple imputation are reviewed, with answers to frequently asked questions about using the method in practice.},
  Doi                      = {10.1191/096228099671525676},
  ISBN                     = {0962-2802 (Print)},
  ISSN                     = {09622802},
  Owner                    = {alyssa},
  Pmid                     = {10347857},
  Timestamp                = {2016.09.27},
  Topics                   = {multiple imputation}
}

@Book{schafer_AIMD1997,
  Title                    = {Analysis of Incomplete Multivariate Data},
  Author                   = {Schafer, J.L.},
  Publisher                = {Chapman and Hall/CRC},
  Year                     = {1997},

  Address                  = {Boca Raton, FL, USA},
  Series                   = {CRC Monographs on Statistics \& Applied Probability},

  Abstract                 = {The last two decades have seen enormous developments in statistical methods for incomplete data. The EM algorithm and its extensions, multiple imputation, and Markov Chain Monte Carlo provide a set of flexible and reliable tools from inference in large classes of missing-data problems. Yet, in practical terms, those developments have had surprisingly little impact on the way most data analysts handle missing values on a routine basis. Analysis of Incomplete Multivariate Data helps bridge the gap between theory and practice, making these missing-data tools accessible to a broad audience. It presents a unified, Bayesian approach to the analysis of incomplete multivariate data, covering datasets in which the variables are continuous, categorical, or both. The focus is applied, where necessary, to help readers thoroughly understand the statistical properties of those methods, and the behavior of the accompanying algorithms. All techniques are illustrated with real data examples, with extended discussion and practical advice. All of the algorithms described in this book have been implemented by the author for general use in the statistical languages S and S Plus. The software is available free of charge on the Internet.},
  Owner                    = {aimbert},
  Timestamp                = {2017.04.11},
  Topics                   = {general}
}

@Article{schafer_graham_PM2002,
  Title                    = {Missing data: our view of the state of the art},
  Author                   = {Schafer, J.L. and Graham, J.W.},
  Journal                  = {Psychological Methods},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {147-177},
  Volume                   = {7},

  Abstract                 = {Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.},
  Doi                      = {10.1037/1082-989X.7.2.147},
  Owner                    = {alyssa},
  Timestamp                = {2017.02.21},
  Topics                   = {general}
}

@Article{schafer_olsen_MBR1998,
  Title                    = {Multiple Imputation for multivariate missing-data problems: a data analyst's perspective},
  Author                   = {Schafer, J.L. and Olsen, M.K.},
  Journal                  = {Multivariate Behavioral Research},
  Year                     = {1998},
  Number                   = {4},
  Pages                    = {545-571},
  Volume                   = {33},

  Abstract                 = {Analyses of multivariate data are frequently hampered by missing values. Until recently, the only missing-data methods available to most data analysts have been relatively ad1 hoc practices such as listwise deletion. Recent dramatic advances in theoretical and computational statistics, however, have produced anew generation of flexible procedures with a sound statistical basis. These procedures involve multiple imputation (Rubin, 1987), a simulation technique that replaces each missing datum with a set of m > 1 plausible values. The rn versions of the complete data are analyzed by standard complete-data methods, and the results are combined using simple rules to yield estimates, standard errors, and p-values that formally incorporate missing-data uncertainty. New computational algorithms and software described in a recent book (Schafer, 1997a) allow us to create proper multiple imputations in complex multivariate settings. This article reviews the key ideas of multiple imputation, discusses the software programs currently available, and demonstrates their use on data from the Adolescent Alcohol Prevention Trial (Hansen & Graham, 199 I).},
  Doi                      = {10.1207/s15327906mbr3304_5},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.14},
  Topics                   = {multiple imputation}
}

@Article{seaman_white_SMMR2011,
  Title                    = {Review of inverse probability weighting for dealing with missing data},
  Author                   = {Seaman, S.R. and White, I.R.},
  Journal                  = {Statistical Methods in Medical Research},
  Year                     = {2011},
  Number                   = {3},
  Pages                    = {278-295},
  Volume                   = {22},

  Abstract                 = {The simplest approach to dealing with missing data is to restrict the analysis to complete cases, i.e. individuals with no missing values. This can induce bias, however. Inverse probability weighting (IPW) is a commonly used method to correct this bias. It is also used to adjust for unequal sampling fractions in sample surveys. This article is a review of the use of IPW in epidemiological research. We describe how the bias in the complete-case analysis arises and how IPW can remove it. IPW is compared with multiple imputation (MI) and we explain why, despite MI generally being more efficient, IPW may sometimes be preferred. We discuss the choice of missingness model and methods such as weight truncation, weight stabilisation and augmented IPW. The use of IPW is illustrated on data from the 1958 British Birth Cohort},
  Doi                      = {10.1177/0962280210395740},
  Owner                    = {alyssa},
  Timestamp                = {2017.03.06},
  Topics                   = {ipw}
}

@Article{simon_simonoff_JASA1986,
  Title                    = {Diagnostic plots for missing data in least squares regression},
  Author                   = {Simon, G.A. and Simonoff, J.S.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1986},
  Number                   = {394},
  Pages                    = {501-509},
  Volume                   = {81},

  Abstract                 = {The usual approach to handling missing data in a regression is to assume that the points are missing at random (MAR) and use either a fill-in method to replace the missing points or a method using maximally available pairs in the sample covariance matrix. We derive limits for the values of the least squares estimates of the coefficients (and their associated t statistics) when there are missing observations in one carrier. These limits are derived subject to a constraint on the relationship of the missing data to the present data. Calculating these limits while varying this constrained value results in a series of diagnostic plots that can be used to study the potential effect of the missing points on the regression (without assuming that the points are MAR). Simulations are performed to illustrate the use of the plots, and two real data sets are analyzed. The more general case of missing data in more than one carrier is also discussed.},
  Doi                      = {10.1080/01621459.1986.10478296},
  Keywords                 = {constrained optimization; missing at random; missing by unknown mechanism; regression diagnostics},
  Owner                    = {nathalie},
  Timestamp                = {2018.04.16},
  Topics                   = {diagnosis}
}

@Article{stacklies_etal_B2007,
  Title                    = {{pcaMethods} -- a bioconductor package providing {PCA} methods for incomplete data},
  Author                   = {Stacklies, W. and Redestig, H. and Scholz, M. and Walther, D. and Selbig, J.},
  Journal                  = {Bioconductor},
  Year                     = {2007},
  Number                   = {9},
  Pages                    = {1164-1167},
  Volume                   = {23},

  Abstract                 = {pcaMethods is a Bioconductor compliant library for computing principal component analysis (PCA) on incomplete data sets. The results can be analyzed directly or used to estimate missing values to enable the use of missing value sensitive statistical methods. The package was mainly developed with microarray and metabolite data sets in mind, but can be applied to any other incomplete data set as well.},
  Doi                      = {10.1093/bioinformatics/btm069},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.09},
  Topics                   = {imputation; factorial data analysis}
}

@Article{stage_crookston_FS2007,
  Title                    = {Partitioning error components for accuracy-assessment of near-neighbor methods of imputation},
  Author                   = {Stage, A.R. and Crookston, N.L.},
  Journal                  = {Forest Science},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {62-72},
  Volume                   = {53},

  Abstract                 = {Imputation is applied for two quite different purposes: to supply missing data to complete a data set for subsequent modeling analyses or to estimate subpopulation totals. Error properties of the imputed values have different effects in these two contexts. We partition errors of imputation derived from similar observation units as arising from three sources: observation error, the distribution of observation units with respect to their similarity, and pure error given a particular choice of variables known for all observation units. Two new statistics based on this partitioning measure the accuracy of the imputations, facilitating comparison of imputation to alternative methods of estimation such as regression and comparison of alternative methods of imputation generally. Knowing the relative magnitude of the errors arising from these partitions can also guide efficient investment in obtaining additional data. We illustrate this partitioning using three extensive data sets from western North America. Application of this partitioning to compare near-neighbor imputation is illustrated for Mahalanobis- and two canonical correlation-based measures of similarity.},
  ISBN                     = {0015-749X},
  ISSN                     = {0015749X},
  Keywords                 = {landscape modeling; missing data; most similar neighbor; k-nn inference},
  Owner                    = {alyssa},
  Timestamp                = {2017.11.13},
  Topics                   = {imputation; kNN; diagnosis}
}

@Article{stekhoven_buhlmann_B2012,
  Title                    = {Missforest-non-parametric missing value imputation for mixed-type data},
  Author                   = {Stekhoven, D.J. and B\"uhlmann, P.},
  Journal                  = {Bioinformatics},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {112-118},
  Volume                   = {28},

  Abstract                 = {MOTIVATION: Modern data acquisition based on high-throughput technology is often facing the problem of missing data. Algorithms commonly used in the analysis of such large-scale data often depend on a complete set. Missing value imputation offers a solution to this problem. However, the majority of available imputation methods are restricted to one type of variable only: continuous or categorical. For mixed-type data, the different types are usually handled separately. Therefore, these methods ignore possible relations between variable types. We propose a non-parametric method which can cope with different types of variables simultaneously. RESULTS: We compare several state of the art methods for the imputation of missing values. We propose and evaluate an iterative imputation method (missForest) based on a random forest. By averaging over many unpruned classification or regression trees, random forest intrinsically constitutes a multiple imputation scheme. Using the built-in out-of-bag error estimates of random forest, we are able to estimate the imputation error without the need of a test set. Evaluation is performed on multiple datasets coming from a diverse selection of biological fields with artificially introduced missing values ranging from 10{\%} to 30{\%}. We show that missForest can successfully handle missing values, particularly in datasets including different types of variables. In our comparative study, missForest outperforms other methods of imputation especially in data settings where complex interactions and non-linear relations are suspected. The out-of-bag imputation error estimates of missForest prove to be adequate in all settings. Additionally, missForest exhibits attractive computational efficiency and can cope with high-dimensional data. AVAILABILITY: The package missForest is freely available from http://stat.ethz.ch/CRAN/.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1105.0828},
  Doi                      = {10.1093/bioinformatics/btr597},
  Eprint                   = {1105.0828},
  ISBN                     = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
  ISSN                     = {13674803},
  Mendeley-groups          = {missing data/random{\_}forest},
  Owner                    = {alyssa},
  Pmid                     = {22039212},
  Timestamp                = {2016.09.27},
  Topics                   = {imputation}
}

@Article{stuart_etal_AJE2009,
  Title                    = {Multiple imputation with large data sets: a case study of the children's mental health initiative},
  Author                   = {Stuart, E.A. and Azur, M. and Frangakis, C. and Leaf, P.},
  Journal                  = {American Journal of Epidemiology},
  Year                     = {2009},
  Number                   = {9},
  Pages                    = {1133-1139},
  Volume                   = {169},

  Abstract                 = {Multiple imputation is an effective method for dealing with missing data, and it is becoming increasingly common in many fields. However, the method is still relatively rarely used in epidemiology, perhaps in part because relatively few studies have looked at practical questions about how to implement multiple imputation in large data sets used for diverse purposes. This paper addresses this gap by focusing on the practicalities and diagnostics for multiple imputation in large data sets. It primarily discusses the method of multiple imputation by chained equations, which iterates through the data, imputing one variable at a time conditional on the others. Illustrative data were derived from 9,186 youths participating in the national evaluation of the Community Mental Health Services for Children and Their Families Program, a US federally funded program designed to develop and enhance community-based systems of care to meet the needs of children with serious emotional disturbances and their families. Multiple imputation was used to ensure that data analysis samples reflect the full population of youth participating in this program. This case study provides an illustration to assist researchers in implementing multiple imputation in their own data.},
  Doi                      = {10.1093/aje/kwp026},
  ISBN                     = {1476-6256 (Electronic)$\backslash$n0002-9262 (Linking)},
  ISSN                     = {00029262},
  Keywords                 = {mental health services; missing at random; missing data; multiple imputation},
  Owner                    = {alyssa},
  Pmid                     = {19318618},
  Timestamp                = {2017.11.08},
  Topics                   = {multiple imputation; chained equations}
}

@Article{su_etal_JSS2011,
  Title                    = {Multiple imputation with diagnostics (mi) in {R}: opening windows into the black box},
  Author                   = {Su, Y.S. and Gelman, A. and Hill, J. and Yajima, M.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2011},
  Pages                    = {2},
  Volume                   = {45},

  Abstract                 = {Our mi package in R has several features that allow the user to get inside the imputation process and evaluate the reasonableness of the resulting models and imputations. These features include: choice of predictors, models, and transformations for chained imputation models; standard and binned residual plots for checking the fit of the conditional distributions used for imputation; and plots for comparing the distributions of observed and imputed data. In addition, we use Bayesian models and weakly informative prior distributions to construct more stable estimates of imputation models. Our goal is to have a demonstration package that (a) avoids many of the practical problems that arise with existing multivariate imputation programs, and (b) demonstrates state-of-the-art diagnostics that can be applied more generally and can be incorporated into the software of others.},
  Doi                      = {10.18637/jss.v045.i02},
  Owner                    = {nathalie},
  Timestamp                = {2017.10.16},
  Topics                   = {multiple imputation; chained equations; diagnosis}
}

@Article{tanner_wong_JASA1987,
  Title                    = {The calculation of posterior distributions by data augmentation},
  Author                   = {Tanner, M.A. and Wong, W.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {1987},
  Number                   = {398},
  Pages                    = {528-540},
  Volume                   = {82},

  Abstract                 = {The idea of data augmentation arises naturally in missing value problems, as exemplified by the standard ways of filling in missing cells in balanced two-way tables. Thus data augmentation refers to a scheme of augmenting the observed data so as to make it more easy to analyze. This device is used to great advantage by the EM algorithm (Dempster, Laird, and Rubin 1977) in solving maximum likelihood problems. In situations when the likelihood cannot be approximated closely by the normal likelihood, maximum likelihood estimates and the associated standard errors cannot be relied upon to make valid inferential statements. From the Bayesian point of view, one must now calculate the posterior distribution of parameters of interest. If data augmentation can be used in the calculation of the maximum likelihood estimate, then in the same cases one ought to be able to use it in the computation of the posterior distribution. It is the purpose of this article to explain how this can be done. The basic idea is quite simple. The observed data y is augmented by the quantity z, which is referred to as the latent data. It is assumed that if y and z are both known, then the problem is straightforward to analyze, that is, the augmented data posterior p(theta | y, z) can be calculated. But the posterior density that we want is p(theta | y), which may be difficult to calculate directly. If, however, one can generate multiple values of z from the predictive distribution p(z | y) (i.e., multiple imputations of z), then p(theta | y) can be approximately obtained as the average of p(theta | y, z) over the imputed z's. However, p(z | y) depends, in turn, on p(theta | y). Hence if p(theta | y) was known, it could be used to calculate p(z | y). This mutual dependency between p(theta | y) and p(z | y) leads to an iterative algorithm to calculate p(theta | y). Analytically, this algorithm is essentially the method of successive substitution for solving an operator fixed point equation. We exploit this fact to prove convergence under mild regularity conditions. Typically, to implement the algorithm, one must be able to sample from two distributions, namely p(theta | y, z) and p(z | theta, y). In many cases, it is straightforward to sample from either distribution. In general, though, either sampling can be difficult, just as either the E or the M step can be difficult to implement in the EM algorithm. For p(theta | y, z) arising from parametric submodels of the multinomial, we develop a primitive but generally applicable way to approximately sample theta. The idea is first to sample from the posterior distribution of the cell probabilities and then to project to the parametric surface that is specified by the submodel, giving more weight to those observations lying closer to the surface. This procedure should cover many of the common models for categorical data. There are several examples given in this article. First, the algorithm is introduced and motivated in the context of a genetic linkage example. Second, we apply this algorithm to an example of inference from incomplete data regarding the correlation coefficient of the bivariate normal distribution. It is seen that the algorithm recovers the bimodal nature of the posterior distribution. Finally, the algorithm is used in the analysis of the traditional latent-class model as applied to data from the General Social Survey.},
  Doi                      = {10.2307/2289457},
  ISSN                     = {01621459},
  Owner                    = {alyssa},
  Publisher                = {American Statistical Association, Taylor \& Francis},
  Timestamp                = {2017.08.31},
  Topics                   = {ML},
  Url                      = {http://www.jstor.org/stable/2289457}
}

@Article{templ_etal_ADAC2012,
  Title                    = {Exploring Incomplete data using visualization techniques},
  Author                   = {Templ, M. and Alfons, A. and Filzmoser, P.},
  Journal                  = {Advances in Data Analysis and Classification},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {29-47},
  Volume                   = {6},

  Abstract                 = {Visualization of incomplete data allows to simultaneously explore the data and the structure of missing values. This is helpful for learning about the distribution of the incomplete information in the data, and to identify possible structures of the missing values and their relation to the available information. The main goal of this contribution is to stress the importance of exploring missing values using visualization methods and to present a collection of such visualization techniques for incomplete data, all of which are implemented in the R package VIM. Providing such functionality for this widely used statistical environment, visualization of missing values, imputation and data analysis can all be done from within R without the need of additional software.},
  Doi                      = {10.1007/s11634-011-0102-y},
  File                     = {:/home/nathalie/Private/Travail/References/Recherche/MissingData/templ_etal_ADAC2012.pdf:PDF},
  Keywords                 = {visualization; missing values; exploring incomplete data; R software},
  Owner                    = {nathalie},
  Timestamp                = {2018.04.11},
  Topics                   = {diagnosis}
}

@Article{thijs_etal_B2002,
  Title                    = {Strategies to fit pattern-mixture models},
  Author                   = {Thijs, H. and Molenberghs, G. and Michiels, B. and Verbeke, G. and Curran, D.},
  Journal                  = {Biostatistics},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {245-265},
  Volume                   = {3},

  Abstract                 = {Whereas most models for incomplete longitudinal data are formulated within the selection model framework, pattern-mixture models have gained considerable interest in recent years (Little, 1993, 1994). In this paper, we outline several strategies to fit pattern-mixture models, including the so-called identifying restrictions strategy. Multiple imputation is used to apply this strategy to realistic settings, such as quality-of-life data from a longitudinal study on metastatic breast cancer patients.},
  Doi                      = {10.1093/biostatistics/3.2.245},
  Keywords                 = {delta method; linear mixed model; missing data; repeated measures; sensitivity analysis},
  Owner                    = {alyssa},
  Timestamp                = {2017.10.11},
  Topics                   = {mnar}
}

@Article{tierney_etal_BMJO2015,
  Title                    = {Using decision trees to understand structure in missing data},
  Author                   = {Tierney, N.J. and Harden, F.A. and Harden, M.J.. and Mengersen, K.L.},
  Journal                  = {BMJ Open},
  Year                     = {2015},
  Number                   = {6},
  Pages                    = {e007450},
  Volume                   = {5},

  Abstract                 = {Objectives Demonstrate the application of decision trees - classification and regression trees (CARTs), and their cousins, boosted regression trees (BRTs) - to understand structure in missing data. Setting Data taken from employees at 3 different industrial sites in Australia. Participants 7915 observations were included. Materials and methods The approach was evaluated using an occupational health data set comprising results of questionnaires, medical tests and environmental monitoring. Statistical methods included standard statistical tests and the 'rpart' and 'gbm' packages for CART and BRT analyses, respectively, from the statistical software 'R'. A simulation study was conducted to explore the capability of decision tree models in describing data with missingness artificially introduced. Results CART and BRT models were effective in highlighting a missingness structure in the data, related to the type of data (medical or environmental), the site in which it was collected, the number of visits, and the presence of extreme values. The simulation study revealed that CART models were able to identify variables and values responsible for inducing missingness. There was greater variation in variable importance for unstructured as compared to structured missingness. Discussion Both CART and BRT models were effective in describing structural missingness in data. CART models may be preferred over BRT models for exploratory analysis of missing data, and selecting variables important for predicting missingness. BRT models can show how values of other variables influence missingness, which may prove useful for researchers. Conclusions Researchers are encouraged to use CART and BRT models to explore and understand missing data.},
  Doi                      = {10.1136/bmjopen-2014-007450},
  Owner                    = {nathalie},
  Timestamp                = {2018.03.30},
  Topics                   = {diagnosis}
}

@Article{troyanskaya_etal_B2001,
  Title                    = {Missing value estimation methods for {DNA} microarrays},
  Author                   = {Troyanskaya, O. and Cantor, M. and Sherlock, G. and Brown, P. and Hastie, T. and Tibshirani, R. and Botstein, D. and Altman, R.B.},
  Journal                  = {Bioinformatics},
  Year                     = {2001},
  Number                   = {6},
  Pages                    = {520-525},
  Volume                   = {17},

  Abstract                 = {Motivation: Gene expression microarray experiments can generate data sets with multiple missing expression values. Unfortunately, many algorithms for gene expression analysis require a complete matrix of gene array values as input. For example, methods such as hierarchical clustering and K-means clustering are not robust to missing data, and may lose effectiveness even with a few missing values. Methods for imputing missing data are needed, therefore, to minimize the effect of incomplete data sets on analyses, and to increase the range of data sets to which these algorithms can be applied. In this report, we investigate automated methods for estimating missing data. Results: We present a comparative study of several methods for the estimation of missing values in gene microarray data. We implemented and evaluated three methods: a Singular Value Decomposition (SVD) based method (SVDimpute), weighted K-nearest neighbors (KNNimpute), and row average. We evaluated the methods using a variety of parameter settings and over different real data sets, and assessed the robustness of the imputation methods to the amount of missing data over the range of 1-20% missing values. We show that KNNimpute appears to provide a more robust and sensitive method for missing value estimation than SVDimpute, and both SVDimpute and KNNimpute surpass the commonly used row average method (as well as filling missing values with zeros). We report results of the comparative experiments and provide recommendations and tools for accurate estimation of missing microarray data under a variety of conditions. Availability: The software is available at http://smi-web.stanford.edu/projects/helix/pubs/impute/},
  Doi                      = {10.1093/bioinformatics/17.6.520},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.14},
  Topics                   = {imputation; kNN}
}

@Article{unnebrink_windeler_SM2001,
  Title                    = {Intention-to-treat: methods for dealing with missing values in clinical trials of progressively deteriorating diseases},
  Author                   = {Unnebrink, K. and Windeler, J.},
  Journal                  = {Statistics in Medecine},
  Year                     = {2001},
  Number                   = {24},
  Pages                    = {3931-3946},
  Volume                   = {20},

  Abstract                 = {Since it came up in the 1960s, the principle of intention-to-treat (ITT) has become widely accepted for the analysis of controlled clinical trials. In this context the question of how to perform such an analysis in the presence of missing information about the main endpoint is of major importance. Uncritical use of several ad hoc strategies for dealing with missing values is common in the practice of clinical trials. On the other hand, little is known about possible dangers and problems of applying these strategies. We therefore performed a detailed investigation of different methods for dealing with missing values in order to develop recommendations for their practical use. A simulation study was performed investigating possible consequences on type I error and power of applying different methods for dealing with missing values. The simulations were based on a clinical trial of osteoporosis, a progressively deteriorating disease. The strategies examined can be roughly classified into numerical imputation strategies (last observation carried forward, mean and regression based methods) and non-parametric strategies (rank and dichotomization based methods). Different drop-out mechanisms and different types of progression of disease are considered. The type I error increases drastically for the different strategies, especially if the courses of disease vary between treatment groups. The loss in power can be substantial. There is no strategy which is adequate for all different combinations of drop-out mechanisms, drop-out rates and courses of disease over time. For drop-out rates less than 20 per cent and similar courses of disease in the treatment groups, missing values might be replaced by the mean of the other group, or counted as treatment failures after dichotomization of the endpoint. For larger drop-out rates or less similar courses of disease, no adequate recommendations can be given. Because of the drastic consequences of increasing drop-out rates, it has to be a primary goal in clinical trials to keep missing values to a minimum. Unobserved information cannot be reliably regained by any methodological resources. As there are no strategies for universal use, reasons for the choice of a certain method have to be provided when designing and analysing clinical trials.},
  Doi                      = {10.1002/sim.1149},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.09},
  Topics                   = {imputation; time series}
}

@Article{vandevelden_bijmolt_P2006,
  Title                    = {Generalized canonical correlation analysis of matrices with missing rows: a simulation study},
  Author                   = {van de Velden, M. and Bijmolt, T.H.A.},
  Journal                  = {Psychometrika},
  Year                     = {2006},

  Abstract                 = {A method is presented for generalized canonical correlation analysis of two or more matrices with missing rows. The method is a combination of Carroll's (1968) method and the missing data approach of the OVERALS technique (Van der Burg, 1988). In a simulation study we assess the performance of the method and compare it to an existing procedure called GENCOM, proposed by Green and Carroll (1988). We find that the proposed method outperforms the GENCOM algorithm both with respect to model fit and recovery of the true structure.},
  Doi                      = {10.1007/s11336-004-1168-9},
  Keywords                 = {generalized canonical correlation analysis,OVERALS,missing values,Monte carlo simulation},
  Owner                    = {nathalie},
  Timestamp                = {2016.09.28},
  Topics                   = {factorial data analysis}
}

@Article{verbanck_etal_SC2015,
  Title                    = {Regularised {PCA} to denoise and visualise data},
  Author                   = {Verbanck, M. and Josse, J. and Husson, F.},
  Journal                  = {Statistics and Computing},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {471-486},
  Volume                   = {25},

  Abstract                 = {Principal component analysis (PCA) is a well-established dimensionality reduction method commonly used to denoise and visualise data. A classical PCA model is the fixed effect model in which data are generated as a fixed structure of low rank corrupted by noise. Under this model, PCA does not provide the best recovery of the underlying signal in terms of mean squared error. Following the same principle as in ridge regression, we suggest a regularised version of PCA that essentially selects a certain number of dimensions and shrinks the corresponding singular values. Each singular value is multiplied by a term which can be seen as the ratio of the signal variance over the total variance of the associated dimension. The regularised term is analytically derived using asymptotic results and can also be justified from a Bayesian treatment of the model. Regularised PCA provides promising results in terms of the recovery of the true signal and the graphical outputs in comparison with classical PCA and with a soft thresholding estimation strategy. The distinction between PCA and regularised PCA becomes especially important in the case of very noisy data.},
  Doi                      = {10.1007/s11222-013-9444-y},
  Keywords                 = {principal component analysis; shrinkage; regularised PCA; fixed effect model; denoising; visualisation},
  Owner                    = {nathalie},
  Timestamp                = {2018.05.09},
  Topics                   = {factorial data analysis; imputation}
}

@Article{verbeke_etal_B2001,
  Title                    = {Sensitivity analysis for nonrandom dropout: a local influence approach},
  Author                   = {Verbeke, G. and Molenberghs, G. and Thijs, H. and Lesaffre, E. and Kenward, M.G.},
  Journal                  = {Biometrics},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {7-14},
  Volume                   = {57},

  Abstract                 = {Diggle and Kenward (1994, Applied Statistics43, 49-93) proposed a selection model for continuous longitudinal data subject to nonrandom dropout. It has provoked a large debate about the role for such models. The original enthusiasm was followed by skepticism about the strong but untestable assumptions on which this type of model invariably rests. Since then, the view has emerged that these models should ideally be made part of a sensitivity analysis. This paper presents a formal and flexible approach to such a sensitivity assessment based on local influence (Cook, 1986, Journal of the Royal Statistical Society, Series B48, 133-169). The influence of perturbing a missing-at-random dropout model in the direction of nonrandom dropout is explored. The method is applied to data from a randomized experiment on the inhibition of testosterone production in rats.},
  Doi                      = {10.1111/j.0006-341X.2001.00007.x},
  ISSN                     = {1541-0420},
  Keywords                 = {compound symmetry; global influence; linear mixed model; missing data; normal curvature},
  Owner                    = {alyssa},
  Publisher                = {Blackwell Publishing Ltd},
  Topics                   = {diagnosis; mnar}
}

@Article{voillet_etal_BMCB2016,
  Title                    = {Handling missing rows in multi-omics data integration: multiple imputation in multiple factor analysis framework},
  Author                   = {Voillet, V. and Besse, P. and Liaubet, L. and San Cristobal, M. and Gonz\'ales, I.},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2016},
  Note                     = {Forthcoming},
  Number                   = {402},
  Volume                   = {17},

  Doi                      = {10.1186/s12859-016-1273-5},
  Owner                    = {nathalie},
  Timestamp                = {2016.09.27},
  Topics                   = {multiple imputation; hot-deck}
}

@Article{vanderwal_geskus_JSS2011,
  Title                    = {{ipw}: an {R} package for inverse probability weighting},
  Author                   = {van der Wal, Willem M. and Geskus, Ronald B.},
  Journal                  = {Journal of Statistical Software},
  Year                     = {2011},
  Number                   = {13},
  Volume                   = {43},

  Abstract                 = {We describe the R package ipw for estimating inverse probability weights. We show how to use the package to fit marginal structural models through inverse probability weighting, to estimate causal effects. Our package can be used with data from a point treatment situation as well as with a time-varying exposure and time-varying confounders. It can be used with binomial, categorical, ordinal and continuous exposure variables.},
  Doi                      = {10.18637/jss.v043.i13},
  Owner                    = {nathalie},
  Timestamp                = {2018.06.07},
  Topics                   = {ipw}
}

@Article{wu_carroll_B1988,
  Title                    = {Estimation and comparison of changes in the presence of informative right censoring by modeling the censoring process},
  Author                   = {Wu, M.C. and Carroll, R.J.},
  Journal                  = {Biometrics},
  Year                     = {1988},
  Number                   = {1},
  Pages                    = {175-188},
  Volume                   = {44},

  Abstract                 = {In the estimation and comparison of the rates of change of a continuous variable between two groups, the unweighted averages of individual simple least squares estimates from each group are often used. Under a linear random effects model, when all individuals have complete observations at identical time points, these statistics are maximum likelihood estimates for the expected rates of change. However, with censored or missing data, these estimates are no longer efficient when compared to generalized least squares estimates. When, in addition, the right-censoring process is dependent on the individual rates of change (i.e., informative right censoring), the generalized least squares estimates will be biased. Likelihood-ratio tests for informativeness of the censoring process and maximum likelihood estimates for the expected rates of change and the parameters of the right-censoring process are developed under a linear random effects model with a probit model for the right-censoring process. In realistic situations, we illustrate that the bias in estimating group rate of change and the reduction of power in comparing group differences could be substantial when strong dependency of the right-censoring process on individual rates of change is ignored.},
  Doi                      = {10.2307/2531905},
  ISSN                     = {0006341X, 15410420},
  Owner                    = {alyssa},
  Timestamp                = {2017.10.25},
  Topics                   = {mnar}
}

@Article{zhang_JSS2012,
  Title                    = {Nearest neighbor selection for iterative {kNN} imputation},
  Author                   = {Zhang, S.},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2012},
  Number                   = {11},
  Pages                    = {2541-2552},
  Volume                   = {85},

  Abstract                 = {Existing kNN imputation methods for dealing with missing data are designed according to Minkowski distance or its variants, and have been shown to be generally efficient for numerical variables (features, or attributes). To deal with heterogeneous (i.e., mixed-attributes) data, we propose a novel kNN (k nearest neighbor) imputation method to iteratively imputing missing data, named GkNN (gray kNN) imputation. GkNN selects k nearest neighbors for each missing datum via calculating the gray distance between the missing datum and all the training data rather than traditional distance metric methods, such as Euclidean distance. Such a distance metric can deal with both numerical and categorical attributes. For achieving the better effectiveness, GkNN regards all the imputed instances (i.e., the missing data been imputed) as observed data, which with complete instances (instances without missing values) together to iteratively impute other missing data. We experimentally evaluate the proposed approach, and demonstrate that the gray distance is much better than the Minkowski distance at both capturing the proximity relationship (or nearness) of two instances and dealing with mixed attributes. Moreover, experimental results also show that the GkNN algorithm is much more efficient than existent kNN imputation methods.},
  Doi                      = {10.1016/j.jss.2012.05.073},
  Keywords                 = {Missing data; k nearest neighbors; kNN imputation},
  Owner                    = {alyssa},
  Timestamp                = {2017.02.20},
  Topics                   = {imputation; kNN}
}

